---
layout: post
title: AIND: 1. Intro to Artificial Intelligence
permalink: /mooc/aind/1
author: Seung Jae Ryan Lee
---

##Intro to Artificial Intelligence

### Navigation

Consider **Navigation**, a problem where given a map, you want to find the shortest path between two cities. This can be solved in many ways. One solution might be **Breadth-First Search (BFS)**, but this is intractable if there are lots of roads. Instead, we can use **A* search**, where we prioritize roads that moves us closer to the goal.

### Tic Tac Toe

The Navigation problem did not require us to adapt to the **environment**. Now, let's consider a different problem: **Tic Tac Toe**. This game is different from navigation since it has another player playing against you.

To use a search algorithm for Tic Tac Toe similar to navigation, we need to represent it as a **graph**. We define the nodes as the entire board and the edges as valid moves. In this representation, edges embody the rules of the game, so there are small (1~8) number of edges.

However, the entire search tree is quite large, so we want to only consider beneficial moves. This is called **Pruning**. Also, we are playing against an opponent which wants to win the game, so we need to consider their winning strategy as well. This is called **Adversarial Search**. In Adversarial Search, you want to maximize your chance of winning. Your opponent however wants to minimize your chance of winning. This can be described with the **Minimax Algorithm**. 

### Monty Hall Problem

The **Monty Hall Problem** is a well-known problem that shows the **complexity and uncertainty of the real world**. Suppose you are part in a game show. There are three doors A, B, C, one with a car behind and two with a goat behind. You select a door (but do not open it). Then the host opens one of the two other doors that has a goat behind. Now, there are two unopened doors left. You have the choice to stay with the original door you picked, and you have the choice to switch to the other door. What should you do?

Surprisingly, the two choices have different probability of winning. For convenience, let's denote the probability that door A has a car behind as $P(Car_A)$, and likewise for B and C. Suppose we choose Door A, and the game host opened Door B. Then, we want to compute $P(Car_A | Open_B)$ and $P(Car_C | Open_B)$. Let us use **Bayes' Theorem**.

$$ P(Car_A | Open_B) = \frac{P(Open_B | Car_A) P(Car_A)}{P(Open_B)}$$

$$ P(Car_C | Open_B) = \frac{P(Open_B | Car_C) P(Car_C)}{P(Open_B)}$$

Notice that $P(Open_B | Car_A)$ and $P(Open_B | Car_C)$ is different. $P(Open_B | Car_A) = 0.5$ since the host can choose between opening door B and C. However, $P(Open_B | Car_C) = 1$ since the host won't open the door with the car behind. Therefore, there is twice the chance that the car is behind door C as opposed to door A. (For actual probability, we need to calculate $P(Open_B)$, which can be done via marginal probability. This is omitted in this post.)

The probability that is "our best guess with no further evidence" is called the **Prior Probability**. (ex. $P(A)$) In contrast, the probability that is "our belief after an observation" is called the **Posterior Probability**. (ex. $P(A | B)$)

### Agent, Environment and State

To design intelligent systems, we need to understand the problem. We call the problem domain the **environment**, and we call the intelligent system itself the **agent**. For example, for floor-cleaning robot, the environment could be the floor and the agent could be the robot. We could consider only the software of the robot to be the agent, but when defining an agent, it is a good idea to include all components that can be reliably controlled.

To complete the task, the agent needs some information. This is called the **state**. For the floor-cleaning robot, the state might be the current position of the robot and the places that has been cleaned. The result that the agent is hoping to achieve is called the **goal state**. Note that there can be multiple goal states.

### Perception, Action and Cognition

The agent understands the environment through its "sensors". This is called **Perception**. The agent then processes the information and chooses an **action** that change the environment. This process of using the information and choosing an action is called **Cognition**. We focus on **Cognition**.

Some agents have very simple algorithms for cognition. This is called **Reactive Agents** or **Behavior Based Agents**.  In contrast, there are agents that use very complex algorithms, which can be classified by their algorithms.

### Types of AI Problems

You can classify AI problems by their environment and the components of state that is captured.

An environment could by **fully observable** or **partially observable**. For example, Tic Tac Toe is fully observable since you can see the entire board, but Battleship is partially observable since you cannot see enemy's ships.

An environment could be **deterministic** or **stochastic**. An environment is deterministic if the results of each action is decided. If there is some uncertainty, the environment is stochastic.

An environment could be **discrete** or **continuous**. An environment is discrete if there is a finite number of states that the environment can be in, and continuous if there are infinitely many.

An environment could be **benign** or **adversarial**. A benign environment is where the agent is the only actor that affect its goal. In contrast, in adversarial environment, there are other agents that try to defeat the goal. Tic Tac Toe would be an adversarial game.

### Rational Behavior and Bounded Optimality

We would want our Artificial Intelligence to solve these types of complex problems. Then, what is **Intelligence**? Perhaps a system that can solve a complex problem might be deemed intelligent. However, we need to be mindful of a few things. If we limit intelligence to systems we do not understand, then everything we create is by definition unintelligent. Also, if we define intelligence as being able to perform well on all tasks, everything that has been created yet is unintelligent. (If you are interested in AI for multiple tasks, this is called General AI.) Therefore, we confine intelligence to a given task.

A widely acceptable definition is that an **intelligent agent** is an agent that has **rational behavior**. Rational behavior is "taking actions to maximize its expected utility given a desired goal." However, this means that agents must behave optimally, and in reality, optimal solutions are extremely difficult to find, and sometimes impossible given the constraints (such as a partially observable environment) that the agent has. Therefore, we allow some error to the agent's behavior or specify a feasible level of performance. This is called **Bounded Optimality**.

