---
layout: post
title: "AIND: 9. Hill Climbing and Simulated Annealing"
permalink: /mooc/aind/9
---

### Iterative Improvement Problems

There are many problems where "adding a little bit of intelligence and iteratively improving the solution" is enough to get close-to-optimal solution. Here are two examples of such problems:

#### Traveling Salesman Problem

The **Traveling Salesman Problem (TSP)** is a problem of trying to find the shortest cycle visiting all nodes of the graph. Finding the solution gets exponentially difficult with respect to the number of cities. However, we can get a solution close to optimal solution by simply drawing a random travel plan and iteratively "uncrossing" two paths that cross each other.

![Traveling Salesman Problem]({{ "assets/mooc/aind/9/tsp.png" | absolute_url }})

#### N-Queens

The **N-Queens Problem** is a problem of putting $N$ queens on a $N \times N$ board so that no queen can attack another queen. Below is the N-Queens problem where $N = 4$.

![N Queens Problem]({{ "assets/mooc/aind/9/4_queens.png" | absolute_url }})

In this position, we have 5 attacks represented by the lines between queens. We found the solution when we have 0 attack. This is our **heuristic function** for this problem.

![Iterative Improvement Step for N Queens]({{ "assets/mooc/aind/9/iterative_improvement.png" | absolute_url }})

We can move a queen in its column to iteratively improve the position. Let's choose a move that will reduce the number of attacks the most. After two steps, we find the solution.

However, there can be a **local minima** in the N-Queens problem. In the board below, there is only one attack, but moving any queen along the column worsens the heuristic function.

![Example of a local minima in N-Queens]({{ "assets/mooc/aind/9/local_minima.png" | absolute_url }})

We will learn different ways to combat this problem of local minima.

### Hill Climbing

![Example of an objective function]({{ "assets/mooc/aind/9/objective_function.png" | absolute_url }})

Let's consider a **Hill Climbing** problem of maximizing the objective function. Suppose that the function has the shape above, and we move along the x-axis.

![Labeled objective function]({{ "assets/mooc/aind/9/labeled_objective_function.png" | absolute_url }})

Because the objective function has multiple local maximum, it is possible that taking small ascending steps will get us stuck in a local maximum, since at the local maximum, there is no ascending step.

### Random Restart

One solution to the local maxima problem is the **Random Restart**. This is a simple solution of starting the iterative improvement step in multiple places, and taking the maximum of all trials.

We can tweak random restart for efficiency. We can track places that have already been visited and restart if we revisit them. This is the **Taboo Search**. Also, we can track all local maxima to predict the general form of the graph This is the **Stage Algorithm**.

### Local Beam Search

Another solution to the local maxima problem is the **Beam Search** algorithm. Instead of tracking one "particle", we keep track of $k$ particles. Every step, we look at the neighbors of these particles.

![Neighbors of particles]({{ "assets/mooc/aind/9/beam_search_neighbors.png" | absolute_url }})

Out of all the neighbors, we choose the $k$ best neighbors and continue.

![Picking best neighbors]({{ "assets/mooc/aind/9/beam_search_best.png" | absolute_url }})

This algorithm is different from random restart since the particles share information.

A variant is the **Stochastic Beam Search**, where we add some randomness when we choose the neighbors to help our chance of finding the global maximum.

### Step Size

Another problem to consider for Hill Climbing is the size of the step. If we take small steps, we might get stuck in a **shoulder**, a place where the hill is momentarily flat. If we take large steps, we could miss the hills completely or oscillate and never converge.

To solve this problem, we will learn the **Simulated Annealing** algorithm in the [next lesson](/mooc/aind/10).

### Simulated Annealing

As the named suggests, simulated annealing borrows the idea of *annealing* from physics. Annealing is a process of a heated substance slowly cooling down. As the molecules's mobility declines, the molecules form a structure with lowest energy configuration. In nature, annealing creates mud cracks or columnar basalts. In **Simulated Annealing**, the "high temperature" equates to high randomness, and the "gradual cooling" is the decrease in randomness.

![Pseudocode for Simulated Annealing]({{ "assets/mooc/aind/9/simulated_annealing.png" | absolute_url }})

In Simulated Annealing, we start at a point and random choose a nearby new position. We compare their values with $\Delta E = E(\text{next}) - E(\text{current}) $, and if the new position is better ($\Delta E > 0$), we move to the new position. However, even if the new position is worse ($\Delta E < 0$), we move to the new position with probability $e^{\Delta E / T}$, where $T$ is the "temperature" of annealing that gradually decreases.

When $T$ is close to infinity, $e^{\Delta E / T} \simeq 1$, so there is a lot of random motion in the beginning. As time passes, $T$ approaches $0$, $\Delta E / T$ becomes small since $\Delta E < 0$, so $e^{\Delta E / T} \simeq 0$. Therefore, in simulated annealing, we start with a lot of random motion, which helps us not get stuck in a local maxima, and we decrease randomness to converge to a maxima.

The simulated annealing algoritm is guaranteed to find the global maximum if we start with high enough $T$ and decrease $T$ slowly enough.
