---
published: false
layout: post
title: "Implementing Deep Q-Networks (DQN) (Mnih et al., 2015)"
author: Seungjae Ryan Lee
permalink: /implementations/dqn
tags:
 - reinforcement-learning
 - implementations

image: /assets/blog/implementations/dqn/TODO.png
image_type: contain
excerpt: "TODO"

nav:
- name: "TODO"
  permalink: "#TODO"

related:
# - title: "RL Weekly 29: The Behaviors and Superstitions of RL, and How Deep RL Compares with the Best Humans"
#   link: /rl-weekly/29
#   image: /assets/blog/rl-weekly/29/bsuite_radar_plot.png
#   image_type: contain
---

## Introduction

Just read the TL;DR at the end of the post.

## Understanding DeepMind's DQN Code

DeepMind used LuaTorch to write DQN.

- [`net_downsample_2x_full_y.lua`](https://github.com/deepmind/dqn/blob/master/dqn/net_downsample_2x_full_y.lua) and [`Scale.lua`](https://github.com/deepmind/dqn/blob/master/dqn/Scale.lua) defines observation preprocessing. (84x84 grayscale image)
- [`convnet_atari3.lua`](https://github.com/deepmind/dqn/blob/master/dqn/convnet_atari3.lua) and [`convnet.lua`](https://github.com/deepmind/dqn/blob/master/dqn/convnet.lua) defines the network architecture.
- [`Rectifier.lua`](https://github.com/deepmind/dqn/blob/master/dqn/Rectifier.lua) cleverly defines the ReLU activation: $relu(x) = (x + \lvert x \rvert)/2$.


### Hyperparameters

Check lines 17 to 45 `train_agent.lua`.

| Name | Variable Name | Value | 
| ---- | ------------- | ----- |
| Env Params | env_params | |
| Frame Pooling | pool_frms | 
| Action Repeat | actrep |
| Random Starts | 0 |
| Agent Params | agent_params | |
| Eval Frequency | eval_freq | |
| Training Steps | 10^5 | |
| Evaluation Steps | 10^5 | |

### Main Loop

Check lines 81 to 212 of `train_agent.lua`.

```
Training for N steps {
    local action_index = agent:perceive(reward, screen, terminal)

    (Line 85-94)
    If game is over, start new game with random start
    Otherwise, take a step - game_env:step(game_actions[action_index], true)
    TODO: nextRandomGame() vs newGame()

    (Line 106-172)
    Every opt.eval_freq after random start, evaluate agent.
        - Start new game (without random start for first game?).
        - Episodes don't end when losing a life
        - Epsilon of 0.05 (TODO Check if it is perceive)
        total_reward = 0
        nrewards = 0
        nepisodes = 0
        episode_reward = 0

        for estep=1,opt.eval_steps do
            Act

            if terminal then
                total_reward = total_reward + episode_reward
                episode_reward = 0
                nepisodes = nepisodes + 1
                screen, reward, terminal = game_env:nextRandomGame()
            end
        end

        eval_time = sys.clock() - eval_time
        start_time = start_time + eval_time
        agent:compute_validation_statistics()
        local ind = #reward_history+1
        total_reward = total_reward/math.max(1, nepisodes)

        if #reward_history == 0 or total_reward > torch.Tensor(reward_history):max() then
            agent.best_network = agent.network:clone()
        end

        if agent.v_avg then
            v_history[ind] = agent.v_avg
            td_history[ind] = agent.tderr_avg
            qmax_history[ind] = agent.q_max
        end
        print("V", v_history[ind], "TD error", td_history[ind], "Qmax", qmax_history[ind])

        reward_history[ind] = total_reward
        reward_counts[ind] = nrewards
        episode_counts[ind] = nepisodes

        time_history[ind+1] = sys.clock() - start_time

        local time_dif = time_history[ind+1] - time_history[ind]

        local training_rate = opt.actrep*opt.eval_freq/time_dif

        print(string.format(
            '\nSteps: %d (frames: %d), reward: %.2f, epsilon: %.2f, lr: %G, ' ..
            'training time: %ds, training rate: %dfps, testing time: %ds, ' ..
            'testing rate: %dfps,  num. ep.: %d,  num. rewards: %d',
            step, step*opt.actrep, total_reward, agent.ep, agent.lr, time_dif,
            training_rate, eval_time, opt.actrep*opt.eval_steps/eval_time,
            nepisodes, nrewards))
    end

    if step % opt.save_freq == 0 or step == opt.steps then
        local s, a, r, s2, term = agent.valid_s, agent.valid_a, agent.valid_r,
            agent.valid_s2, agent.valid_term
        agent.valid_s, agent.valid_a, agent.valid_r, agent.valid_s2,
            agent.valid_term = nil, nil, nil, nil, nil, nil, nil
        local w, dw, g, g2, delta, delta2, deltas, tmp = agent.w, agent.dw,
            agent.g, agent.g2, agent.delta, agent.delta2, agent.deltas, agent.tmp
        agent.w, agent.dw, agent.g, agent.g2, agent.delta, agent.delta2,
            agent.deltas, agent.tmp = nil, nil, nil, nil, nil, nil, nil, nil

        local filename = opt.name
        if opt.save_versions > 0 then
            filename = filename .. "_" .. math.floor(step / opt.save_versions)
        end
        filename = filename
        torch.save(filename .. ".t7", {agent = agent,
                                model = agent.network,
                                best_model = agent.best_network,
                                reward_history = reward_history,
                                reward_counts = reward_counts,
                                episode_counts = episode_counts,
                                time_history = time_history,
                                v_history = v_history,
                                td_history = td_history,
                                qmax_history = qmax_history,
                                arguments=opt})
        if opt.saveNetworkParams then
            local nets = {network=w:clone():float()}
            torch.save(filename..'.params.t7', nets, 'ascii')
        end
        agent.valid_s, agent.valid_a, agent.valid_r, agent.valid_s2,
            agent.valid_term = s, a, r, s2, term
        agent.w, agent.dw, agent.g, agent.g2, agent.delta, agent.delta2,
            agent.deltas, agent.tmp = w, dw, g, g2, delta, delta2, deltas, tmp
        print('Saved:', filename .. '.t7')
        io.flush()
        collectgarbage()
    end
end
```





## Implementing DQN in PyTorch

## Conclusion

**TL;DR**




