---
layout: post
title: "AIND: 17. Exact Inference: Enumeration"
permalink: /mooc/aind/17
---

In the [last lesson](/mooc/aind/16), we learned about Bayes networks: a compact way to specify joint probability distributions. In this lesson, we will learn how to compute conditional probabilities with the Bayes network. This is called **inference**. We will learn about **enumeration**, one way to perform inference.

### Inference

With a Bayes network, we can compute probability distributions of unobserved variables given some known variables. We call the known (observed) variables the **Evidence** variables, and we call the unknown (unobserved) variables the **Query** variables. Other variables that are neither query nor evidence are called the **Hidden** variables.

Note that based on the inference problem, the division of Query, Hidden, Evidence variables change. If we want to compute $P(B \mid A)$, $A$ is the evidence node and $B$ is the query node, but if we want to compute $P(A \mid B)$, $B$ is the evidence, and $A$ is the query.

There are two types of inference for Bayes networks. One is to compute the probability distribution of query variables given the evidence variables. 

$$ P(Q_1, Q_2, \ldots \mid E_1 = e_1, E_2 = e_2, \ldots) $$

The other type is the most likely values for query variables given the evidence variables.

$$ \text{argmax}_q P(Q_1 = q_1, Q_2 = q_2, \ldots \mid E_1 = e_1, E_2 = e_2, \ldots) $$

### Enumeration

Consider the Bayes network below. If there is a burglary or an earthquake, it is likely that an alarm will ring. If the alarm rings, then John and Mary who might hear the alarm will give a call.

![Example of Bayes network]({{ "assets/AIND-17/bayes_network_ex.png" | absolute_url }})

One simple inference method is **Enumeration**: a method of going through all possible. Suppose that we want to find $P(B = +b \mid J = +j, M = +m)$: the probability of a burglary given that John and Mary called.

The first step of enumeration is to divide the conditional probability to unconditional probability:

$$ P(+b \mid +j, +m) = P(+b, +j, +m) / P(+j, +m) $$

Then, we can compute these unconditional probabilities by considering all possible values for the hidden variables:

$$ P(+b, +j, +m) = \sum_e \sum_a P(+b, +j, +m, e, a) $$

$$ P(+j, +m) = \sum_b \sum_e \sum_a P(b, +j, +m, e, a) $$

Then, we can use the conditional independence from Bayes network:

$$  P(+b, +j, +m) = \sum_e \sum_a P(+b) P(e) P(a \mid +b, e) P(+j \mid a) P(+m \mid a) $$

$$ P(+j, +m) = \sum_b \sum_e \sum_a P(b) P(e) P(a \mid b, e) P(+j \mid a) P(+m \mid a) $$

Then, with the conditional probability tables specified for the Bayes network, we can compute the probabilities.

$$ P(+b \mid +j, +m) \simeq 0.284 $$

### Speeding Enumeration

Enumeration is impractical for bigger Bayes networks. For the example above, we only needed to sum over two or three variables, but for $n$ variables, we have $2^n$ combinations of values (assuming boolean variables). Also, the number of probabilities to multiply also grows.

One optimization to enumeration is simply pulling out unaffected terms from the sum. For example in the equations above, $P(b)$ does not depend on $e$ or $a$, and $P(e)$ doesn't depend on $a$, so they can be pulled out of the sum.

$$
P(+b, +j, +m) = P(+b) \sum_e P(e) \sum_a P(a \mid +b, e) P(+j \mid a) P(+m \mid a)
$$

![Evaluation Tree for enumeration]({{ "assets/AIND-17/evaluation_tree.png" | absolute_url }})

The tree above shows how $P(+b, +j, +m)$ will be calculated. Although it is better than naive enumeration, it still calculates a few terms multiple times. To avoid such unneeded calculations, we use a different method called variable elimination.

### Sparse Representations

Another optimization to enumeration is to maximize independence of the Bayes network.

![Alternative Bayes network for same problem]({{ "assets/AIND-17/alternative_bayes_network.png" | absolute_url }})

Consider a network above. We created this network by adding the variables in order $J, M, A, B, E$. This network has two more edges than the previous network, so it is a denser network. In denser networks we can pull out less terms because there are more dependencies, so the number of computations increase.

To generate the most sparse network, we need to add the variables in **Causal Direction**: causes should come before effects. In this example, $B, E$ causes $A$, and $A$ causes $J,M$, so the order would be $B, E, A, J, M$. Indeed, this order generates the first network, which is the best (most sparse) network.

