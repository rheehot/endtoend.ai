---
layout: post
title: "AIND: 18. Exact Inference: Variable Elimination"
permalink: /mooc/aind/18
---

In the [last lesson](/mooc/aind/17), we learned about enumeration, the simplest way to perform inference in Bayes network. In this lesson, we will learn about variable elimination, a more efficient way for inference.

### Variable Elimination

Enumeration worked well for a simple Bayes network, but for a bigger network, we need a more efficient inference algorithm. We will use a **Variable Elimination** algorithm: a dynamic programming algorithm that eliminates repetitive calculations.

![Another example of Bayes network]({{ "assets/AIND-18/bayes_network_ex_2.png" | absolute_url }})

Suppose a new Bayes network, where $R$ denotes probability of rain, $T$ denotes probability of traffic jam, and $L$ denotes probability of being late to work. Let's try to calculate the probability of being late $P(+l)$. We can calculate this with enumeration:
$$
P(+l) = \sum_r \sum_t   P(+l \mid t) P(t \mid r) P(r)
$$
For variable elimination, we define **factors**. A factor is a matrix indexed by the values of its argument variables. In other words, it contains probabilities for all possible values of random variables. For example,
$$
f_2(R, T) = \begin{pmatrix}
P(+t \mid +r) & P(+t \mid -r) \\
P(-t \mid +r) & P(-t \mid -r)
\end{pmatrix} = 
\begin{pmatrix}
0.8 & 0.1 \\
0.2 & 0.9
\end{pmatrix}
$$
We put subscripts on factors to distinguish them.
$$
P(+l) = \sum_t P(+l \mid t) \sum_r P(t \mid r) P(r) = \sum_t f_1(L, T) \sum_r f_2(R, T) f_3(R)
$$
Then, we join multiple factors in a sum to create a new factor. We join factors right-to-left, so we first join $f_3(R) = P(R)$ and $f_2(R, T) = P(T \mid R)$. First, we compute the **pointwise product** $\times$ of factors $f_2, f_3$. The pointwise product is the operation of computing the joint probability $P(R, T)$ through $P(R), P(T \mid R)$, 

![Joining P(R) and P(T|R)]({{ "assets/AIND-18/factor_join.png" | absolute_url }})

Then, we **eliminate** a variable by considering all possible values of that variable and summing the factor's value for each possible value. $\sum_t f_2(R, T) f_3(T) = f_2(+r, T) f_3(+r) + f_2(-r, T) f_3(-r)$. Knowing the joint probability $P(R, T)$ we calculated above, we can compute the sum and eliminate the variable $R$. Meanwhile, we give the new probability $P(T)$ the label $f_4$.

![Eliminate R]({{ "assets/AIND-18/marginal_prob.png" | absolute_url }})

Now, the problem is reduced to
$$
P(L) = \sum_t P(L \mid T) P(T) = \sum_t f_1(L, T) f_4(T)
$$
Similarly, we first join $f_1, f_4$ to compute the joint probability $P(T, L)$. Then, we eliminate $T$ by summing over different values of $T$. 

![Variable Elimination Repeated]({{ "assets/AIND-18/elimination_again.png" | absolute_url }})

Let's go back to the burglary example. We want to calculate the following probability:
$$
P(B \mid +j, +m) = \alpha P(+b) \sum_e P(e) \sum_a P(a \mid +b, e) P(+j \mid a) P(+m \mid a)
$$
Note that we calculate $P(B \mid +j, +m)$ by calculating $P(B, +j, +m)$ and normalizing the probabilities for each value of $B$. We can rewrite the terms with factors:
$$
P(B \mid +j, +m) = \alpha f_1(B) \sum_e f_2(E) \sum_a f_3(A, B, E) f_4(A) f_5(A)
$$
Note that $f_4$ and $f_5$ don't have $J, M$ as arguments since they are given. First, we create a new factor $f_6$ by joining $f_3, f_4, f_5$ and summing out variable $A$:
$$
P(B \mid +j, +m) = \alpha f_1(B) \sum_e f_2(E) f_6(B, E)
$$
Then, we create a new factor $f_7$ by joining $f_2, f_6$ and summing out variable $E$:
$$
P(B \mid +j, +m) = \alpha f_1(B) f_7(B)
$$
Finally, we take the pointwise product of $f_1, f_7$, which gives the probability $P(B, +j , +m)$. We can normalize this to find $P(B \mid +j, +m)$. 

It might not be obvious at first that this method sped up the process at all. In fact, we are using the same equation as the enumeration method. However, calculating the probabilities right-to-left and saving the probabilities as matrices prevented us from repeatedly computing $P(j \mid a)$ and $P(m \mid a)$. In this case, the probabilities were part of the Bayes network so the performance gain might not be obvious, but for larger networks, avoiding such repetitions could lead to a noticeable result.

