---
published: false
layout: post
title: "RL Weekly 8: "
author: Seungjae Ryan Lee
permalink: /rl-weekly/8
tags:
 - reinforcement-learning
 - rl-weekly

front_image: /assets/blog/rl-weekly/7/hanabi.png
meta_image: /assets/blog/rl-weekly/7/hanabi.png
front_image_type: contain
front_text: >
   This week, we introduce the Obstacle Tower Challenge, a new RL competition by Unity, Hanabi Learning Environment, a multi-agent environment by DeepMind, and Spinning Up Workshop, a workshop hosted by OpenAI.
excerpt: >
   This week, we introduce the Obstacle Tower Challenge, a new RL competition by Unity, Hanabi Learning Environment, a multi-agent environment by DeepMind, and Spinning Up Workshop, a workshop hosted by OpenAI.

nav:
- name: "World Discovery Models"
  permalink: '#world-discovery-models'
- name: "MuJoCo Soccer Environment"
  permalink: '#mujoco-soccer-environment'
- name: "Hyperparamter Optimization On the Fly (HOOF)"
  permalink: '#hyperparameter-optimization-on-the-fly'
---



## World Discovery Models

<div class="w50" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/8/ndigo.png" alt="Global Architecture of the NDIGO Agent">
</div>

**What it is**

DeepMind introduced a new agent incorporates Neural Differential Information Gain Optimisation (NDIGO) to build an accurate world model. NDIGO measures novelty through high helpful a new observation was in predicting future observations. The authors show that NDIGO outperforms state of the art methods in terms of the quality of the learned representation.

**Why it matters**

Although reinforcement learning has achieved remarkable success, it is largely restricted to environments where a good external reward in provided.

**Read more**

- [World Discovery Models (ArXiv Preprint)](https://arxiv.org/abs/1902.07685)





## MuJoCo Soccer Environment

<div class="w100" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/8/soccer.png" alt="MuJoCo Soccer Environment">
</div>

**What it is**

Google DeepMind open-sourced a new multi-agent environment (2-vs-2) where agents play soccer on a simulated physics environment. The authors also provide a baseline that uses population-based training, reward shaping, policy recurrence, and decomposed action-value function.

**Why it matters**

The traditional reinforcement learning benchmark has been the Atari 2600 games with the Arcade Learning Environment. However, as of 2019, it is safe to say that the benchmark has served its main purpose, as reinforcement learning agents have achieved superhuman scores on almost all games. Thus, a more challenging testbed is now needed to accelerate progress. DeepMind seems to be interested on multiagent reinforcement learning environments, open sourcing Capture the Flag, Hanabi, and Soccer environment.

**Read more**

- [Emergent Coordination Through Competition (ArXiv Preprint)](https://arxiv.org/abs/1902.07151)
- [Emergent Coordination through Competition Sample Gameplay (Google Sites)](https://sites.google.com/view/emergent-coordination/home)
- [DeepMind MuJoCo Multi-Agent Soccer Environment (GitHub)](https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer)

**External Resources**

- [Arcade Learning Environment (GitHub)](https://github.com/mgbellemare/Arcade-Learning-Environment)
- [Capture the Flag: the emergence of complex cooperative agents (DeepMind Blog)](https://deepmind.com/blog/capture-the-flag/)
- [The Hanabi Challenge: A New Frontier for AI Research (arXiv Preprint)](https://arxiv.org/abs/1902.00506)


## Hyperparamter Optimization On the Fly

<div class="w50" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/8/" alt="">
</div>

**What it is**

**Why it matters**

**Read more**

- [Fast Efficient Hyperparameter Tuning for Policy Gradients (ArXiv Preprint)](https://arxiv.org/abs/1902.06583)




---

Here are some other exciting news:

- Google Brain achieved new state of the art on weakly-supervised semantic parsing with [Meta Reward Learning (MeRL)](https://arxiv.org/abs/1902.07198).
- OpenAI trained a large-scale unsupervised language model called [GPT-2](https://blog.openai.com/better-language-models/) and released some of their models. OpenAI's decisions have generated a lot of controversy, including a panel by [This Week in ML & AI (TWiML&AL)](https://www.youtube.com/watch?v=LWDbAoPyQAk).
- UC Berkeley's CS294-158 course on [Deep Unsupervised Learning](https://sites.google.com/view/berkeley-cs294-158-sp19/home) by Pieter Abbeel started.
