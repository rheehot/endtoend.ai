---
layout: post
title: "RL Weekly 3: Learning to Drive through Dense Traffic, Learning to Walk, and Summarizing Progress in Sim-to-Real"
author: Seungjae Ryan Lee
permalink: /rl-weekly/3
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/3/deeptraffic.png
front_image_type: cover
excerpt: >   
   In this issue, we introduce the DeepTraffic competition from Lex Fridman's MIT Deep Learning for Self-Driving Cars course. We also review a new paper on using SAC to control a four-legged robot, and introduce a website summarizing progress in sim-to-real algorithms.

nav:
- name: DeepTraffic
  permalink: '#deeptraffic-learning-to-drive-through-dense-traffic'
- name: Learning to Walk
  permalink: '#learning-to-walk-via-deep-reinforcement-learning'
- name: Progress in Sim2Real
  permalink: '#progress-in-sim2real'
---



## DeepTraffic: Learning to Drive through Dense Traffic

<div class="w60" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/3/deeptraffic.png" alt="Visualization of a DeepTraffic submission">
</div>

**What it is**

"DeepTraffic is a deep reinforcement learning competition part of the MIT Deep Learning for Self-Driving Cars course. The goal is to create a neural network to drive a vehicle (or multiple vehicles) as fast as possible through dense highway traffic." You are given a fully working code of Deep Q-Network (DQN), and the car also comes with a safety system, so the network only has to tell the car if it should accelerate/slow down or change lanes.

**Why it matters**

The participants of the competition have limited freedom: the competition has a preset algorithm. Thus, the participants can only change the hyperparameters of the model: the network architecture, learning rate, batch size, discount factor, experience replay, etc.

Still, the competition is particularly attractive for two reasons: 1) it has a small discretized state space and action space and 2) it is a "fun" and no-install environment where you can train an agent to control a self-driving car in a web browser. Having small state space and action space, it only takes 5 to 10 minutes to train a decent agent that drives over 60mph. This allows you to test your intuition on important hyperparameters quickly. To drive well, how spatially wide should the agent's observation be? How far into the future should the agent look ahead? How complex should the neural network be? These questions allow you to grow an intuition on the impact of some sensitive hyperparameters, and such intuition will prove to be helpful when you debug your agent in other environments.

**Read more**

- [DeepTraffic (Official website)](https://selfdrivingcars.mit.edu/deeptraffic/)
- [DeepTraffic Leaderboard (Official website)](https://selfdrivingcars.mit.edu/deeptraffic-leaderboard/)
- [DeepTraffic Documentation (Official website)](https://selfdrivingcars.mit.edu/deeptraffic-documentation/)
- [DeepTraffic (GitHub)](https://github.com/lexfridman/deeptraffic)
- [DeepTraffic (arXiv paper)](https://arxiv.org/abs/1801.02805)

**External Resources**

- [MIT Deep Learning (Official website)](https://deeplearning.mit.edu/)
- [Human-level control through Deep Reinforcement Learning (DeepMind Blog Post)](https://deepmind.com/research/dqn/)
- [Human-level control through Deep Reinforcement Learning (Nature Paper)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)



## Learning to Walk via Deep Reinforcement Learning

<div class="w60" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/3/minotaur.png" alt="Stability and robustness of Minotaur robot trained with this algorithm">
</div>

**What it is**

This paper proposes a variant of the Soft Actor Critic (SAC) algorithm and shows that it achieves state-of-the-art performance in benchmarks and showcase a four-legged robot trained with this algorithm.

Traditional reinforcement learning suffers from lack of robustness: changing a single hyperparameter can drastically worsen the performance of the agent. In the case of SAC, one such hyperparameter is the temperature: the parameter that defines the trade-off between exploration and exploitation. The authors propose a gradient-based temperature tuning method instead.

The algorithm is used to train a four-legged Minotaur robot to walk on a terrain. Without any simulation or pretraining, the robot learns to walk with just 2 hours of training in real-world training time. Furthermore, despite being trained only on a flat terrain, the robot is able to walk up and down the slope, ram through obstacles, and step down a set of stairs.

**Why it matters**

As discussed above, some hyperparameters are incredibly sensitive. Unfortunately, most deep reinforcement learning algorithms contain many few sensitive hyperparameters, and these often need to be tuned differently for different environments. Being able to remove a hyperparameter without sacrificing any performance is certainly a nice improvement, as it eases the computational resources needed to train and tune SAC.

**Read more**

- [Learning to Walk via Deep Reinforcement Learning (arXiv paper)](https://arxiv.org/abs/1812.11103)
- [Learning to Walk via Deep Reinforcement Learning (Google Sites)](https://sites.google.com/view/minitaur-locomotion/)

**External Resources**

- [RL Weekly 1: Soft Actor-Critic Code Release; Text-based RL Competition; Learning with Training Wheels](/rl-weekly/1)
- [Soft Actor Critic - Deep Reinforcement Learning with Real-World Robots (BAIR Blog Post)](https://bair.berkeley.edu/blog/2018/12/14/sac/)
- [Soft Actor-Critic Algoritms and Applications (Project Website)](https://sites.google.com/view/sac-and-applications)
- [Soft Actor-Critic Algoritms and Applications (Paper)](https://drive.google.com/file/d/1J8gZXJN0RqH-TkTh4UEikYSy8AqPTy9x/view)



## Progress in Sim2Real

<div class="w30" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/3/sim2real.png" alt="Sim2Real">
</div>

**What it is**

Last year, many papers in reinforcement learning were focused on using reinforcement learning to control real-world robots. Due to the inefficiency of reinforcement learning algorithms, it is not possible to train them in the real world. Thus, a popular strategy is to train the agent with a simulated environment and  transfer the policy to the real world. This is known as sim-to-real.

Sim2Real is a website created in January 1, 2019 to "[index] the progress on simulations to real world transfer in robot perception and control [motivated] by the recent interest in learning in simulated environments and transferring results to the real world."

**Why it matters**

Reinforcement learning is a field with rapid progress: most state-of-the-art methods are challenged in a few months and defeated in less than a year. This progress shows the high potential of reinforcement learning, but it can also be very overwhelming. Because most new papers claim state-of-the-art, it is difficult to select papers with effective and novel ideas. We believe that the Sim2Real website shows great promise in clarifying the progress in the field of sim-to-real.

**Read more**

- [sim2RealAI (Official website)](https://sim2realai.github.io/)
- [sim2real (Twitter)](https://twitter.com/sim2realAIorg)
- [2018 Sim2Real Round up (Twitter thread)](https://twitter.com/sim2realAIorg/status/1072017025805910017)



---

First Textworld Problems competition is starting on January 7th! Check the [first issue](/rl-weekly/1) to learn more about the competition.

Louis Kirsch created a nice map of the goals, methods, and challenges of reinforcement learning. Check his blog post [here](http://louiskirsch.com/maps/reinforcement-learning)!

