---
layout: post
title: "RL Weekly 19: Curious Object-Based Search Agent, AutoRL, and Multiplicative Compositional Policies"
author: Seungjae Ryan Lee
permalink: /rl-weekly/19
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/19/todo.png
image_type: contain
excerpt: "This week, we introduce combining unsupervised learning, exploration, and model-based RL; evolving rewards; and learning composable m,motor skills."

nav:
- name: "COBRA"
  permalink: "#cobra"
- name: "AutoRL"
  permalink: "#autorl"
- name: "MCP"
  permalink: "#mcp"

related:
- title: "RL Weekly 18: Survey of Domain Randomization Techniques for Sim-to-Real Transfer, and Evaluating Deep RL with ToyBox"
  link: /rl-weekly/18
  image: /assets/blog/rl-weekly/19/cobra.png
  image_type: contain
---



{% include revue.html %}





## COBRA: Curious Object-Based seaRch Agent {#cobra}

Nicholas Watters\*, Loic Matthey\*, Matko Bo≈°njak, Christopher P. Burgess, and Alexander Lerchner (DeepMind)

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/19/cobra.png" alt="">
</div>

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/19/cobra_perf.png" alt="">
</div>

**What it is**

Researchers at DeepMind proposed a new agent COBRA, which incorporates object-based representation learning, curiosity-driven exploration, and model-based RL. COBRA is trained in two phases: the exploration phase and the task phase. In the exploration phase, the agent learns in a task-free, reward-free unsupervised learning setting. Then, in the task phase, the agent is trained on the task through model-based RL.

As shown in the diagram above, COBRA has four components: the vision module (scene encoder and decoder), the transition model, the exploration policy, and reward predictor. The first three are trained during the exploration phase and is freezed during the task phase. During the task phase, only the reward predictor is trained to solve a task.

MONet is used for the vision module, distributional RL for the exploration policy, and 1-step Model Predictive Control (MPC) for the reward predictor.

**Why it matters**

By using unsupervised learning with exploration to learn a object-based representation, COBRA can achieve both high data efficiency and robust policy. The second figure above shows its robustness and data efficiency.

We recommend the readers to read the Supplementary material section for a great detailed decussion on the training details, variations considered, and ablation studies.

**Read more**

- [COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration (ArXiv Preprint)](https://arxiv.org/abs/1905.09275)
- [COBRA agent videos (Google Drive)](https://drive.google.com/drive/folders/1JgBPltIB2E8b_RffcvLpEzc50kjj8QBG?usp=sharing)

**External Resources**

- [MONet: Unsupervised Scene Decomposition and Representation (ArXiv Preprint)](https://arxiv.org/abs/1901.11390)
- [Implicit Quantile Networks for Distributional Reinforcement Learning (Arxiv Preprint)](https://arxiv.org/abs/1806.06923)









## MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies {#mcp}

Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, Sergey Levine (UC Berkeley)

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/19/mcp_tasks.png" alt="">
</div>

**What it is & Why it matters**

In a multi-task RL framework, the agent needs to learn reusable skills from "pre-training tasks" and use them in the "transfer tasks". One way to train such agent is using a Mixture-Of-Experts (MOE) model, a Hierarchical RL method where the policy is represented as a weighted sum of a set of low-level policies ("primitives"). This "additive model" is limited to only one primitive per timestep, which could be deter the agent solving  complex tasks that requires several subtasks being performed at the same time.

Researchers at UC Berkeley proposed Multiplicative Compositional Policies (MCP), a method of factoring agent's behavior into primitives with respect to action space and enabling the agent to use multiple primitives at the same time. MCP is shown to have superior performance compared to other hierarchical models such as Option-Critic or Mixture-Of-Experts (MOE).

**Read more**

- [MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (ArXiv Preprint)](https://arxiv.org/abs/1905.09808)
- [MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (Website)](https://xbpeng.github.io/projects/MCP/)
- [MCP: Multiplicative Compositional Policies (YouTube Video)](https://www.youtube.com/watch?v=ChxSx8-sX_c)





## Evolving Rewards to Automate Reinforcement Learning {#autorl}

Aleksandra Faust, Anthony Francis, Dar Mehta (Robotics at Google)


**What it is**

Researchers at Google Robotics automated reward search with "AutoRL". Proposed in their preceding work "Learning Navigation Behaviors End-to-End with AutoRL," AutoRL is an evolutionary reward search method where it finds the reward parameters that maximizes the task objective. In other words, it "treats reward tuning as hyperparameter optimization."

**Why it matters**

Reward shaping has been a difficult problem in RL. Rewards could be too sparse for the algorithms to discover, or misshaped to encourage faulty behavior. For example, for the Humanoid task, we want the agent to move by walking smoothly like a human, not rolling on the ground. AutoRL is an attempt to replace this complex hand-tuning of rewards with automated reward shaping.

**Read more**

- [Learning Navigation Behaviors End-to-End with AutoRL (ArXiv Preprint)](https://arxiv.org/abs/1809.10124)
- [Evolving Rewards to Automate Reinforcement Learning (ArXiv Preprint)](https://arxiv.org/abs/1905.07628)
- [Evolving Rewards to Automate Reinforcement Learning (YouTube Video)](https://www.youtube.com/watch?v=svdaOFfQyC8)






---

Some more exciting news in RL:

- Researchers at Google proved [theoretical bounds for average-reward RL setting when there is no prior knowledge about the mixing time of the Markov chain](https://arxiv.org/abs/1905.09704).
- Researchers at Technion Israel Institute of Technology, Google Research, and DeepMind proposed a new [Sparse Imitation Learning (Sparse-IL)](https://arxiv.org/abs/1905.09700) algorithm that solves the text-based game Zork1.
- Researchers at Technion and DeepMind [introduced Uncertainty Robust Bellman Equation (URBE) and proposed DQN-URBE](https://arxiv.org/abs/1905.08188), an algorithm that is robust yet not overly conservative.
- Researchers at Google Robotics used [Hierarchical RL to successfully train a quadruped robot](https://arxiv.org/abs/1905.08926).
