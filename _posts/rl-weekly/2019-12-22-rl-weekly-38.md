---
layout: post
title: "RL Weekly 38: Clipped objective is not why PPO works, and the Trap of Saliency maps"
author: Seungjae Ryan Lee
permalink: /rl-weekly/38
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/38/table23.png
image_type: contain
excerpt: "In this issue, we look at the effect of PPO's code-level optimizations and the study of saliency maps in RL."

nav:
- name: "Implementation Matters in Deep RL: A Case Study on PPO and TRPO"
  permalink: "#implementation-matters"
- name: "Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep RL"
  permalink: "#saliency-in-rl"
- name: "More News"
  permalink: "#more-news"

related:
- title: "RL Weekly 37: Observational Overfitting, Hindsight Credit Assignment, and Procedurally Generated Environment Suite"
  link: /rl-weekly/37
  image: /assets/blog/rl-weekly/37/obs_overfit.png
  image_type: contain
---


{% include revue.html %}

<style>
.letter, .letter p {
  color: gray;
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
.letter a {
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
</style>

<div class="letter">
<p>
Dear readers,
</p>
<p>
Happy holidays! Right after NeurIPS 2019, now the ICLR 2020 results are out! Here are two papers accepted to ICLR 2020 that caught my attention.
</p>
<p>
I wait for your feedback, either by <a href="mailto:seungjaeryanlee@gmail.com">email</a> or <a href="https://forms.gle/yZiHUXbtph8msVHn9">a feedback form</a>. Your input is always appreciated.
</p>
<p>
- Ryan
</p>
</div>



## Implementation Matters in Deep RL: A Case Study on PPO and TRPO {#implementation-matters}

<p class="authors" style="font-size: 1em">
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry
<!-- Author<sup>1</sup>, -->
</p>
<!-- <p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Group
</p> -->

<div class="w50">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/38/definition.png" alt="">
</div>

<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/38/table23.png" alt="">
</div>

**What it says**

Proximal Policy Optimization (PPO) has been used over Trust Region Policy Optimization (TRPO) due to its simplicity and performance. However, compared to TRPO, PPO has many more "code-level optimizations" such as value function clipping, reward scaling, learning rate annealing, etc. These optimizations make PPO difficult to reproduce, as they are only briefly mentioned in the paper or only mentioned in the code. The authors find that these techniques are actually critical to PPO. In an ablation study, the code-level optimizations have a bigger impact on performance than the clipped objective. Furthermore, the experiments show that these optimizations are also crucial to maintaining the trust region.

**Read more**

- [ICLR2020 OpenReview](https://openreview.net/forum?id=r1etN1rtPB)

**External resources**

- [Trust Region Policy Optimization (arXiv Preprint)](https://arxiv.org/abs/1502.05477)
- [Proximal Policy Optimization Algorithms (arXiv Preprint)](https://arxiv.org/abs/1707.06347)
- [OpenAI Baselines (GitHub Repo)](https://github.com/openai/baselines)






## Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep RL {#saliency-in-rl}

<p class="authors" style="font-size: 1em">
Akanksha Atrey<sup>1</sup>,
Kaleigh Clary<sup>1</sup>,
David Jensen<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>University of Massachusetts Amherst
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/38/saliency-in-rl.png" alt="">
</div>

**What it says**

Saliency maps have been used in RL for qualitative analysis. For example, in Atari Breakout, saliency maps have been shown as evidence for the claim that the agent has found the "tunnel" strategy (clearing small column of bricks) In Figure (a) above, the saliency map is red near the tunnel, denoting high salience. However, the authors find that just by moving the tunnel horizontally, the saliency pattern disappears.

The authors argue that saliency maps have been used in a subjective manner that cannot be falsified with experiments and perform case studies with saliency maps on Atari Breakout and Atari Amidar. The authors conclude that saliency maps should not be used to infer explanations, but as an exploratory tool to formulate hypothesis.

**Read more**

- [arXiv Preprint](https://arxiv.org/abs/1912.05743)
- [ICLR2020 OpenReview](https://openreview.net/forum?id=rkl3m1BFDB)













------

<div id="more-news"></div>

Here are some more exciting news in RL:

[**OpenAI Five**](https://openai.com/projects/five/)
<br/>
The whitepaper for OpenAI Five, the Dota 2 AI system that defeated the best human team, has been uploaded to arXiv.

[**The PlayStation Reinforcement Learning Environment (PSXLE)**](https://arxiv.org/abs/1912.06101)
<br/>
A new environment suite made by modifying a PlayStation emulator.

[**Learning Human Objectives by Evaluating Hypothetical Behavior**](https://arxiv.org/abs/1912.05652)
<br/>
Model the reward function by generating trajectories and asking users to label synthesized transitions with reward.

<!-- Here are also some applications of RL applied to:
- [Heating Systems](https://arxiv.org/abs/1912.05313)
- [Coded Caching](https://arxiv.org/abs/1912.04321)
- [Traffic Coordination in Intersections](https://arxiv.org/abs/1912.03851)
- [Job Scheduling](https://arxiv.org/abs/1912.05160)
- [Symbolic Regression](https://arxiv.org/abs/1912.04871)
- [Limit Order Book in Stock Market](https://arxiv.org/abs/1912.04242)
- [Readability Assessment](https://arxiv.org/abs/1912.05957)
- [Pedestrian Path Planning](https://arxiv.org/abs/1912.02945) -->

<!-- [**TODO**](todo)
<br/>
TODO -->

