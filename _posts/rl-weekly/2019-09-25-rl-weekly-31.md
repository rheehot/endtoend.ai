---
layout: post
title: "RL Weekly 31: How Agents Play Hide and Seek, Attraction-Repulsion Actor Critic, and Efficient Learning from Demonstrations"
author: Seungjae Ryan Lee
permalink: /rl-weekly/31
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/31/hide-and-seek-front.png
image_type: cover
excerpt: "In this issue, we look at OpenAI's work on multi-agent hide and seek and the behaviors that emerge. We also look at Mila's population-based exploration that exceeds the performance of various TD3 and SAC baselines. Finally, we look at DeepMind's R2D3, a new algorithm to learn from demonstrations."

nav:
- name: "Emergent Tool Use from Multi-Agent Autocurricula"
  permalink: "#hide-and-seek"
- name: "Attraction-Repulsion Actor Critic"
  permalink: "#arac"
- name: "Making Efficient Use of Demonstrations to Solve Hard Exploration Problems"
  permalink: "#r2d3"
- name: "More News"
  permalink: "#more-news"

related:
- title: "RL Weekly 30: Learning State and Action Embeddings, a New Framework for RL in Games, and an Interactive Variant of Question Answering"
  link: /rl-weekly/30
  image: /assets/blog/rl-weekly/30/dyne.png
  image_type: contain
- title: "RL Weekly 32: New SotA Sample Efficiency on Atari and an Analysis of the Benefits of Hierarchical RL"
  link: /rl-weekly/32
  image: /assets/blog/rl-weekly/32/laser_sample_efficiency.png
  image_type: contain
---


{% include revue.html %}



## Emergent Tool Use from Multi-Agent Autocurricula {#hide-and-seek}

<p class="authors" style="font-size: 1em">
Bowen Baker<sup>1*</sup>,
Ingmar Kanitscheider<sup>1*</sup>,
Todor Markov<sup>1*</sup>,
Yi Wu<sup>1*</sup>,
Glenn Powell<sup>1*</sup>,
Bob McGrew<sup>1*</sup>,
Igor Mordatch<sup>2*</sup>,
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>OpenAI,
    <sup>2</sup>Google Brain
</p>


<div class="w100">
  <div class="youtube-responsive">
    <iframe src="https://www.youtube.com/embed/kopoLzvh5jY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><br/>
  </div>
</div>
<div class="w90">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/31/hide-and-seek-1.gif" alt="">
</div>
<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/31/hide-and-seek-2.gif" alt="">
</div>


**What it says**

Consider a simulated environment of a two-team hide-an-seek game, where the hiders (blue) are tasked to avoid being seen by the seekers, whereas the seekers (red) are tasked to keep vision of the hiders. What makes the problem interesting is the fact that their are obstacles and objects in the environment that the hiders and seekers can use. Also, the hiders are given a "preparation phase" where they can change the environment to their advantage while the seekers are immobilized. Changing the environment can be done in two ways: moving the objects around and locking objects in place. The reward scheme is simple: if all hiders are hidden from the seekers, the hiders get +1 reward and the seekers get -1 reward. If at least 1 hider is visible to the seekers, the seekers get +1 reward and the hiders get -1 reward.

In this simple environment, complex behavior emerge as the episode progresses. At first, the hiders simply hide from the seekers without using the objects. Then, they start using obstacles and objects during the preparation phase to create a "shelter." When the seekers learn to breach this shelter using the ramp object, the hiders then learn to bring the ramp into the shelter or lock the ramps far away from the shelter so that the seekers cannot use them. The seekers then learn to "box surf," moving on top of the box object, to breach the shelter, where the hiders react by locking all objects during the preparation phase.

Agents are trained using self-play, and the agent policies are trained with Proximal Policy Optimization (PPO). "All agents share the same policy parameters but act and observe independently." The training is distributed using the rapid framework, and needs 132.3 million episodes (31.7 billion frames) over 34 hours of training to reach the "final stage" of observed agent behaviors.

I recommend those who are interested to start by watching the short YouTube Video above, then reading the [post in the OpenAI website](https://openai.com/blog/emergent-tool-use/).

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/1909.07528)
- [YouTube Video](https://www.youtube.com/watch?v=kopoLzvh5jY)
- [Twitter Tweet](https://twitter.com/OpenAI/status/1174815179483172864)
- [OpenAI Website](https://openai.com/blog/emergent-tool-use/)
- [GitHub Repo: Multiagent Emergence Environments](https://github.com/openai/multi-agent-emergence-environments)
- [GitHub Repo: Worldgen: Randomized MuJoCo environments](https://github.com/openai/mujoco-worldgen)




## Attraction-Repulsion Actor Critic for Continuous Control Reinforcement Learning (ARAC) {#arac}

<p class="authors" style="font-size: 1em">
Thang Doan<sup>12*</sup>,
Bogdan Mazoure<sup>12*</sup>,
Audrey Durand<sup>23</sup>,
Joelle Pineau<sup>124</sup>,
R Devon Hjel<sup>256</sup>,
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>McGill University,
    <sup>2</sup>Mila,
    <sup>3</sup>Université Laval,
    <sup>4</sup>Facebook AI Research,
    <sup>5</sup>MSR Montreal,
    <sup>6</sup>Université de Montréal
</p>

<div class="w70">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/31/arac-figure.png" alt="">
</div>
<div class="w30">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/31/arac-term.png" alt="">
</div>

**What it says**

Large action spaces are problematic for reinforcement learning, as they can lead to local optimas in dense-reward environments. In these environments, exploration is necessary during training to better cover the action space. The authors propose a new population-based exploration method, where a population of agents cover different parts of the action space. In other words, we want agents that can imitate a target policy using a different path.

Through the Attraction-Repulsion (AR) auxiliary loss shown above, the population of agents are encouraged to attract or repel one another. The AR loss measures the KL-divergence of two policies. If the coefficient in front of the KL divergence is positive, the agents "repel", and if it is negative, the agents "attract" one another. To calculate the AR loss of a policy, the algorithm keeps a fixed-size policy archive (Section 3.1).

The training of Attraction-Repulsion Actor-Critic (ARAC) is a repetition of sample collection, critic updates, actor updates, and actor evaluation. To update the critic network, ARAC identifies top-K best agent ("elites") and only uses these policies. Also, when updating the actor networks, only the top-K agents use the AR loss (Appendix 6.1).

In MuJoCo benchmarks, ARAC is shown to outperform CEM-TD3, CERL, ERL, SAC-NF, SAC, and TD3 in most tasks.

As a sidenote, the paper mentions the Machine Learning Reproducibility Checklist in its appendix and points out how the paper satisfies each point.

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/1909.07543)

**External resources**

- [Variational Inference with Normalizing Flows: ArXiv Preprint](https://arxiv.org/abs/1505.05770)
- [The Machine Learning Reproducibility Checklist](https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf)




## Making Efficient Use of Demonstrations to Solve Hard Exploration Problems {#r2d3}

<p class="authors" style="font-size: 1em">
Tom Le Paine<sup>1*</sup>,
Caglar Gulcehre<sup>1*</sup>,
Bobak Shahriari<sup>1</sup>,
Misha Denil<sup>1</sup>,
Matt Hoffman<sup>1</sup>,
Hubert Soyer<sup>1</sup>,
Richard Tanburn<sup>1</sup>,
Steven Kapturowski<sup>1</sup>,
Neil Rabinowitz<sup>1</sup>,
Duncan Williams<sup>1</sup>,
Gabriel Barth-Maron<sup>1</sup>,
Ziyu Wang<sup>1</sup>,
Nando de Freitas<sup>1</sup>,
Worlds Team<sup>1</sup>,
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>DeepMind, London
</p>

<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/31/r2d3.png" alt="">
</div>
<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/31/r2d3-hard-eight.png" alt="">
</div>

**What it says**

Learning from demonstrations is very effective in many hard exploration environments (such as *Montezuma's Revenge*) and helps reducing sample efficiency. However, it is challenging to use demonstrations if there is a wide variety of possible initial conditions, since the learner must generalize between these different environment configurations.

To solve these types of problems, the authors propose a new algorithm: Recurrent Replay Distributed DQN from Demonstrations (R2D3). As the name suggests, the system is similar to that of Recurrent Replay Distributed DQN (R2D2) and Deep Q-Learning from Demonstrations (DQfD). As in R2D2, there are multiple recurrence actor processes that run independently, adding transitions to the shared experience replay buffer. To incorporate demonstrations like DQfD, there is another replay buffer, the demo replay buffer, where expert demonstrations are stored. Using a "demo-ratio" hyperparameter, the agent uses experience from both replay buffers.

R2D3 is tested on the "Hard-Eight test suite" (Appendix B.1) with three baseline algorithm: Behavior Cloning, R2D2, and DQfD. R2D3 is the only algorithm that solves any of the tasks, showing that aspects from both R2D2 and DQfD are needed to succeed in these environments. The authors also find that the algorithm in sensitive to the "demo-ratio" hyperparameter and that the lower demo-ratios consistently outperform higher demo-ratios (Section 6.2, Figure 6).

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/1909.01387)
- [DeepMind Website](https://deepmind.com/research/publications/Making-Efficient-Use-of-Demonstrations-to-Solve-Hard-Exploration-Problems)

**External resources**

- [Recurrent Experience Replay in Distributed Reinforcement Learning (ArXiv Preprint)](https://openreview.net/forum?id=r1lyTjAqYX)
- [Deep Q-learning from Demonstrations (ArXiv Preprint)](https://arxiv.org/abs/1704.03732)





------

<div id="more-news"></div>

Here are some more exciting news in RL since the last issue:

- [**AC-Teach: A Bayesian Actor-Critic Method for Policy Learning with an Ensemble of Suboptimal Teachers**](https://arxiv.org/abs/1909.04121): Learn from multiple suboptimal teachers that solve parts of the problem or contradict one another.
- [**Animal-AI Environment**](https://arxiv.org/abs/1909.07483): The whitepaper for the environment used for NeurIPS 2019 Animal-AI Olympics has been uploaded to arXiv.
- [**DeepGait: Planning and Control of Quadrupedal Gaits using Deep Reinforcement Learning**](https://arxiv.org/abs/1909.08399): Train a four-legged robot for various non-flat terrains through a high-level terrain-aware Gait Planner and a low-level Gait Controller.
- [**Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control**](https://arxiv.org/abs/1909.02682): Limit the variance of messages in Multi-agent RL to improve communication efficiency.
- [**Evolution Strategies**](https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html): A blog post on Evolutionary Strategies and how it can be used in deep RL.
- [**Meta-Inverse Reinforcement Learning with Probabilistic Context Variables**](https://arxiv.org/abs/1909.09314): Learn reward functions from a set of unstructured demonstrations that can generalize to new tasks with a single demonstration.
- [**Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning**](https://meta-world.github.io): Open-source simulated benchmark for meta-RL and multitask learning.
- [**rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch**](https://github.com/astooke/rlpyt): A PyTorch implementation of common deep RL algorithms.
- [**VILD: Variational Imitation Learning with Diverse-quality Demonstrations**](https://arxiv.org/abs/1909.06769): Model the level of expertise of the demonstrators to learn from demonstrations of diverse quality.


<!-- - [**Blackbox Attacks on Reinforcement Learning Agents Using Approximated Temporal Information**](https://arxiv.org/abs/1909.02918): Adversarial samples can be used to create a "time bomb," triggering an agent to misbehave after a specific delay. -->
<!-- - [**Meta-Learning with Implicit Gradients**](https://arxiv.org/abs/1909.04630) -->
<!-- - [**Fixed-Horizon TD**](https://arxiv.org/abs/1909.03906): Predict cumulative reward of a fixed horizon to avoid  -->
<!-- - [Model Based Planning with Energy Based Models](https://arxiv.org/abs/1909.06878) -->
<!-- - [**Expert-Level Atari Imitation Learning from Demonstrations Only**](https://arxiv.org/abs/1909.03773) -->
<!-- - [**Learning Transferable Domain Priors for Safe Exploration in Reinforcement Learning**](https://arxiv.org/abs/1909.04307): Deduce generally undesirable actions from previously learned tasks for safer RL. -->
<!-- - [**DRLViz**](https://arxiv.org/abs/1909.02982) -->

<!-- **Applications to diverse fields:**
- [**Aerial Base Station Placement**](https://arxiv.org/abs/1909.08093)
- [**Automatic Configuration of Energy Harvesting Sensor**](https://arxiv.org/abs/1909.01968)
- [**Automatic Diagnosis of Acute Appendicitis in Abdominal CT**](https://arxiv.org/abs/1909.00617)
- [**Control of Probabilistic Boolean Networks**](https://arxiv.org/abs/1909.03331)
- [**Data Center Job Scheduling**](https://arxiv.org/abs/1909.07820)
- [**Driving in Dense Traffic**](https://arxiv.org/abs/1909.06710)
- [**Dynamic Pricing of Express Lanes with Multiple Access Locations**](https://arxiv.org/abs/1909.04760)
- [**Flight Control**](https://arxiv.org/abs/1909.06493)
- [**Image Captioning**](https://arxiv.org/abs/1909.03622)
- [**Learning to Rank for Information Retrieval**](https://arxiv.org/abs/1909.06859)
- [**Maximum Cut Problem**](https://arxiv.org/abs/1909.04063)
- [**Multi-Fingered Adaptive Tactile Grasping**](https://arxiv.org/abs/1909.04787)
- [**No-Press Diplomacy**](https://arxiv.org/abs/1909.02128)
- [**Off-road Autonomous Vehicles Traversability Analysis and Trajectory Planning**](https://arxiv.org/abs/1909.06953)
- [**Physical Activity Suggestion**](https://arxiv.org/abs/1909.03539)
- [**Semantic Segmentation**](https://arxiv.org/abs/1909.04108)
- [**Software-Defined Networks Control**](https://arxiv.org/abs/1909.01544)
- [**Solving Continual Combinatorial Selection**](https://arxiv.org/abs/1909.03638)
- [**Summarization**](https://arxiv.org/abs/1909.01610)
- [**Text-based Games**](https://arxiv.org/abs/1909.01646)
- [**Tracking Control of Autonomous Underwater Vehicles**](https://arxiv.org/abs/1909.03204)
- [**Transfer Learning**](https://arxiv.org/abs/1908.11406)
- [**Visual Navigation with Natural Multimodal Assistance**](https://arxiv.org/abs/1909.01871) -->

Thank you for reading RL Weekly. [Please feel free to leave any feedback!](https://forms.gle/yZiHUXbtph8msVHn9)
