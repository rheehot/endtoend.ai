---
layout: post
title: "RL Weekly 15: Learning without Rewards: from Active Queries or Suboptimal Demonstrations"
author: Seungjae Ryan Lee
permalink: /rl-weekly/15
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/15/viceraq.png
image_type: contain
excerpt: "In this issue, we introduce VICE-RAQ by UC Berkeley and T-REX by UT Austin and Preferred Networks. VICE-RAQ trains a classifier to infer rewards from goal examples and active querying. T-REX learns reward functions from suboptimal demonstrations ranked by humans."

nav:
- name: "RL without Reward Engineering"
  permalink: "#end-to-end-robotic-rl-without-reward-engineering"
- name: "Beyond Suboptimal Demos"
  permalink: "#extrapolating-beyond-suboptimal-demonstrations"

related:
- title: "RL Weekly 14: OpenAI Five and Berkeley Blue"
  link: /rl-weekly/14
  image: /assets/blog/rl-weekly/14/five.png
  image_type: cover
---


## End-to-End Robotic RL without Reward Engineering

<div class="w60" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/15/viceraq.png" alt="VICE-RAQ">
</div>

**What it is**

Researchers at UC Berkeley proposed VICE-RAQ, a learning algorithm that replaces the need for rewards with a number of goal examples and active binary feedback from humans during training. This algorithm is an improvement to Variational Inverse Control with Events (VICE), where a neural network classifier is trained with dataset of goal examples given and bad examples generated by the policy currently being trained. The proposed Reinforcement Learning with Active Queries (RAQ) method allows the agent to seldom ask humans whether its attempt was a success or a failure. With their extension to off-policy, VICE-RAQ with Soft Actor Critic can successfully learn various robot manipulation tasks.

**Why it matters**

Reinforcement learning algorithms are designed to maximize cumulative reward, so the reward signal is a crucial part of reinforcement learning. However, in the real world designing a reward function is complicated. First, it is difficult to design a dense reward signal since it is often uncertain what behavior should be rewarded. Also, detecting such behavior is difficult and requires multiple sensors. To give reward when the robot has completed a task, there must be sensors to verify that the robot has indeed completed the task. There have been various classifier-based approaches to replace reward signals with raw images of goal examples, since they require no such sensors. VICE-RAQ offers a method where the human can assist the agent in such situations.

**Read more**

- [End-to-End Robotic Reinforcement Learning without Reward Engineering (ArXiv Preprint)](https://arxiv.org/abs/1904.07854)
- [End-to-End Robotic Reinforcement Learning without Reward Engineering (Google Sites)](https://sites.google.com/view/reward-learning-rl/)
- [End-to-End Robotic Reinforcement Learning without Reward Engineering (YouTube Video)](https://www.youtube.com/watch?v=9pWJzb4G-CA)
- [avisingh599/reward-learning-rl (GitHub Repo)](https://github.com/avisingh599/reward-learning-rl)

**External Resources**

- [Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition (ArXiv Preprint)](https://arxiv.org/abs/1805.11686)

## Extrapolating Beyond Suboptimal Demonstrations

<div class="w60" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/15/trex.png" alt="T-REX">
</div>

**What it is**

Researchers at UT Austin and Preferred Networks developed a new inverse RL (IRL) method that can significantly outperform the demonstrator. Their new algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), requires a ranked list of demonstrations. With these ranked demonstrations, T-REX trains the reward function with a loss function that signals that demonstrations with better rank should have better total reward. T-REX performs better than existing IRL methods: Behavioral Cloning with Observations (BCO) and Generative Adversarial Imitation Learning (GAIL).

Note that T-REX can be seen as a type of preference-based inverse RL (PBIRL).

**Why it matters**

Collecting demonstrations is difficult, as it requires an expert playing on the same reinforcement learning environment. Yet, there is no guarantee that they will be perfect, as various factors such as reaction speed or human error is possible. Thus, it might not be beneficial for the agent to similar "imitate" every human action. T-REX attempts to learn from suboptimal demonstrations through preference-based inverse RL, and shows its capabilities with  results in various Atari and MuJoCo domains.


**Read more**

- [Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations (ArXiv Preprint)](https://arxiv.org/abs/1904.06387)

**External Resources**

- [Behavioral Cloning from Observation (ArXiv Preprint)](https://arxiv.org/abs/1805.01954)
- [Generative Adversarial Imitation Learning (ArXiv Preprint)](https://arxiv.org/abs/1606.03476)
- [A Survey of Preference-Based Reinforcement Learning Methods (JMLR)](http://jmlr.org/papers/v18/16-634.html)

---

Some more exciting news in RL:

- [OpenAI Five Arena](https://arena.openai.com/#/results) is almost over! OpenAI Five currently has 4394 wins and 41 losses, with one team managing to defeat OpenAI Five 10 times in a row.
- u/Kuvster98 shared a [simple demo of a trained RL agent landing a simulated rocket](https://1329d3ec-452a-48f4-8599-2f1102f4bdbb.htmlpasta.com/).
- Curt Park open-sourced [Rainbow-IQN](https://www.reddit.com/r/MachineLearning/comments/bewcp6/p_rainbowiqn_that_reaches_the_perfect_score_21_on/).
