---
layout: post
title: "RL Weekly 30: Learning State and Action Embeddings, a New Framework for RL in Games, and an Interactive Variant of Question Answering"
author: Seungjae Ryan Lee
permalink: /rl-weekly/30
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/30/dyne.png
image_type: contain
excerpt: "In this issue, we look at a representation learning method to train state and action embeddings paired with TD3. We also look at a new framework with 20+ environments and algorithms from multiple fields of RL. Finally, we look at a new take on using RL for Question Answering (QA)."

nav:
- name: "Notice"
  permalink: "#notice"
- name: "Dynamics-aware Embeddings"
  permalink: "#dyne"
- name: "OpenSpiel: RL Game Framework"
  permalink: "#openspiel"
- name: "Interactive Machine Reading Comprehension"
  permalink: "#imrc"
- name: "More News"
  permalink: "#more-news"

related:
- title: "RL Weekly 29: The Behaviors and Superstitions of RL, and How Deep RL Compares with the Best Humans"
  link: /rl-weekly/29
  image: /assets/blog/rl-weekly/29/bsuite_radar_plot.png
  image_type: contain
---


{% include revue.html %}


## Notice

Thank you for reading RL Weekly. I created a Google Form to make it easier to leave feedback. [Please feel free to leave any feedback!](https://forms.gle/yZiHUXbtph8msVHn9)

Next week, I will be flying back to Princeton to finish my bachelor's degree, so issue #31 could be delayed. I hope to bring you more exciting news in reinforcement learning in the future!

\- Ryan


## Dynamics-aware Embeddings {#dyne}

<p class="authors" style="font-size: 1em">
William Whitney<sup>13</sup>,
Rajat Agarwal<sup>1</sup>,
Kyunghyun Cho<sup>13</sup>,
Abhinav Gupta<sup>23</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Department of Computer Science, New York University,
    <sup>2</sup>Robotics Institute, Carnegie Mellon University,
    <sup>3</sup>Facebook AI Research
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/30/dyne.png" alt="">
</div>

<div class="w70">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/30/dyne_variational_lower_bound.png" alt="">
</div>

**What it says**

*"The intuition is simple: preserve as much information as possible about the \*outcomes\* of a state or action while minimizing its description length. This leads to embeddings with smooth structure that make it easy to do RL."*
\- William Whitney, the first author.

Most popular RL algorithms are trained end-to-end: the convolutional layers that extract features are trained with the fully connected layers that choose action or estimate values. Instead of this end-to-end approach, the authors propose a new representation learning objective DynE (Dynamics-Aware Embedding). DynE learns a state encoder and an action encoder by maximizing a variational lower bound (Section 2.2), then learns an action decoder that produces low-level action sequence from a high-level action (Section 3.1). With a simple modification (Section 3.2), DynE can be used with TD3 (Twin-delayed DDPG).

DynE-TD3 is tested with Reacher and 7DoF tasks from MuJoCo. DynE-TD3 learns faster that other baselines (PPO, TD3, SAC, and SAC-LSP) and its action representations are transferable to similar environments (Page 8, Figure 4). The authors also compare using just the state embedding (S-Dyne-TD3) and using both embeddings (SA-Dyne-TD3). In simpler environments, both achieve similar performance, but SA-Dyne-TD3 shines in harder environment. Both perform better than TD3 and VAE-TD3 (Page 9, Figure 5).

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/1908.09357)
- [GitHub Repository](https://github.com/willwhitney/dynamics-aware-embeddings)
- [Twitter Thread by Will Whitney (First Author)](https://twitter.com/wfwhitney/status/1166475418548998149)

**External resources**

- [[PPO] Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- [[TD3] Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477)
- [[SAC] Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290)
- [[SAC-LSP] Latent Space Policies for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1804.02808)






## OpenSpiel: A Framework for Reinforcement Learning in Games {#openspiel}

<p class="authors" style="font-size: 1em">
Marc Lanctot<sup>1*</sup>,
Edward Lockhart<sup>1*</sup>,
Jean-Baptiste Lespiau<sup>1*</sup>,
Vinicius Zambaldi<sup>1*</sup>,
Satyaki Upadhyay<sup>2</sup>,
Julien Pérolat<sup>1</sup>,
Sriram Srinivasan<sup>2</sup>,
Finbarr Timbers<sup>1</sup>,
Karl Tuyls<sup>1</sup>,
Shayegan Omidshafiei<sup>1</sup>,
Daniel Hennes<sup>1</sup>,
Dustin Morrill<sup>13</sup>,
Paul Muller<sup>1</sup>,
Timo Ewalds<sup>1</sup>,
Ryan Faulkner<sup>1</sup>,
János Kramár<sup>1</sup>,
Bart De Vylder<sup>1</sup>,
Brennan Saeta<sup>2</sup>,
James Bradbury<sup>2</sup>,
David Ding<sup>1</sup>,
Sebastian Borgeaud<sup>1</sup>,
Matthew Lai<sup>1</sup>,
Julian Schrittwieser<sup>1</sup>,
Thomas Anthony<sup>1</sup>,
Edward Hughes<sup>1</sup>,
Ivo Danihelka<sup>1</sup>,
Jonah Ryan-Davis<sup>2</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>DeepMind,
    <sup>2</sup>Google,
    <sup>3</sup>University of Alberta
</p>

<div class="w100">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/30/openspiel.png" alt="">
</div>

**What it says**

"OpenSpiel is a collection of environments and algorithms for research in general RL and search/planning in games." It contains 28 environments (shown above) that have various different properties: perfect or imperfect information, single-agent or multi-agent, etc. It also contains 22 baseline algorithms that spans a wide field of RL such as tree search, tabular RL, deep RL, and multi-agent RL. The environments are implemented in C++ and wrapped in Python, and algorithms are implemented in either C++ or Python. For visualization and evaluation of multi-agent RL, OpenSpiel offers phase portraits (Section 3.3.1) and $\alpha$-Rank (Section 3.3.2).

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/1908.09453)
- [GitHub Repository](https://github.com/deepmind/open_spiel)
- [Reddit Discussion (r/reinforcementlearning)](https://www.reddit.com/r/reinforcementlearning/comments/cw6t45/openspiel_new_deepmind_multigameenvironment_rl/)
- [Twitter Thread by Marc Lanctot (First author)](https://twitter.com/sharky6000/status/1166343247133724673)

**External resources**

- [VentureBeat Article](https://venturebeat.com/2019/08/27/deepmind-details-openspiel-a-collection-of-ai-training-tools-for-video-games/)






## Interactive Machine Reading Comprehension {#imrc}

<p class="authors" style="font-size: 1em">
Xingdi Yuan<sup>1*</sup>,
Jie Fu<sup>2*</sup>,
Marc-Alexandre Cote<sup>1</sup>,
Yi Tay<sup>3</sup>,
Christopher Pal<sup>2</sup>,
Adam Trischler<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Microsoft Research, Montreal,
    <sup>2</sup>Polytechnique Montreal, Mila,
    <sup>3</sup>Nanyang Technological University
</p>

<div class="w50">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/30/imrc_example.png" alt="">
</div>

<div class="w50">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/30/imrc.png" alt="">
</div>


**What it says**

Question Answering (QA) is a problem where the model must output a correct answer given a question and a supporting paragraph containing the answer. In the web, the text to search the answer for could be very large, so the authors instead propose an "interactive" variant. Unlike the default "static" situation where the full text is given, in the interactive variant only a part of the text is given and the agent must select actions to discover sentences with relevant information. During this information gathering phase, the agent can choose to move to the previous or next sentence, to jump to a next occurrence of a token (i.e. Ctrl+F), or to stop. When a stop action is given, the agent enters the question answering phase in which it outputs the head and tail position of the answer in the supporting text.

The authors build interactive versions of SQuAD v1.1 and NewsQA and test their methods and report F1 scores of 0.666 for iSQuAD and 0.367 for iNewsQA (best numbers among variants, Table 4). The authors also report 0.716 and 0.632 for the "F1 info score," which only tests whether the agent stopped at the correct segment of the text during the information gathering phase.

**Read more**

- [ArXiv Preprint](https://arxiv.org/abs/1908.10449)

**External resources**

- [SQuAD Leaderboard](https://rajpurkar.github.io/SQuAD-explorer/): XLNet F1 95.080, Human F1 91.221
- [[SQuAD] ArXiv Preprint](https://arxiv.org/abs/1606.05250)
- [[NewsQA] ArXiv Preprint](https://arxiv.org/abs/1611.09830)





------

<div id="more-news"></div>

More exciting news in RL this week:

**Algorithms**
- [KNN-based Continuous Value Iteration with "Imaginary" Data Augmentation](https://arxiv.org/abs/1908.10255)
- [Transfer Learning in Text Adventure Games using Knowledge Graphs](https://arxiv.org/abs/1908.06556)

**Survey of...**
- [Probabilistic Graphical Model and Variational Inference](https://arxiv.org/abs/1908.09381)
- [RL Applications](https://arxiv.org/abs/1908.06973)
- [Intrinsic Motivation in RL](https://arxiv.org/abs/1908.06976)

**Applying RL for:**
- [Anomaly Detection](https://arxiv.org/abs/1908.10755)
- [Networked Control Systems](https://arxiv.org/abs/1908.10722)
