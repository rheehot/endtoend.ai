---
layout: post
title: "RL Weekly 20: Minecraft Competition, Off-policy Policy Evaluation via Classification, and Soft-attention Agent for Interpretability"
author: Seungjae Ryan Lee
permalink: /rl-weekly/20
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/20/minerl.png
image_type: contain
excerpt: "This week, we introduce MineRL, a new RL competition using human priors to solve Minecraft. We also introduce OPE, a method of off-policy evaluation through classification, and a soft-attention agent for greater interpretability."

nav:
- name: "MineRL"
  permalink: "#minerl"
- name: "Off-policy Policy Evaluation"
  permalink: "#ope"
- name: "Interpretable RL with Attention"
  permalink: "#attention"

related:
- title: "RL Weekly 19: Curious Object-Based Search Agent, Multiplicative Compositional Policies, and AutoRL"
  link: /rl-weekly/19
  image: /assets/blog/rl-weekly/19/cobra.png
  image_type: contain
- title: "RL Weekly 21: The interplay between Experience Replay and Model-based RL"
  link: /rl-weekly/21
  image: /assets/blog/rl-weekly/21/sorb.png
  image_type: contain
---



{% include revue.html %}





## MineRL: Learn Minecraft from Human Priors {#minerl}

<p class="authors" style="font-size: 0.8em">
William H. Guss<sup>*1</sup>,
Cayden Codel<sup></sup>,
Katja Hofmann<sup>2</sup>,
Brandon Houghton<sup>1</sup>,
Noboru Kuno<sup>2</sup>,
Stephanie Milani<sup>3</sup>,
Sharada Mohanty<sup>4</sup>,
Diego Perez Liebana<sup>5</sup>,
Ruslan Salakhutdinov<sup>1</sup>,
Nicholay Topin<sup>1</sup>,
Manuela Veloso<sup>1</sup>,
Phillip Wang<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 0.8em">
    <sup>1</sup>Carnegie Mellon University
    <sup>2</sup>Microsoft Research
    <sup>3</sup>University of Maryland
    <sup>4</sup>AICrowd
    <sup>5</sup>Queen Mary University of London
</p>

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/20/minerl.gif" alt="MineRL Competition Task">
</div>

**What it is**

MineRL is a competition for the upcoming NeurIPS 2019 conference. The competition uses the Minecraft environment, and the goal of the participants is to train the agent to obtain diamonds. This is a very difficult task, so the organizers also provide the MineRL dataset, which is a large-scale dataset of human demonstrations.

The competition started a few days ago and will end on October 25th. According to the organizers, Preferred Networks will be releasing a set of baselines for the competition soon.

**Why it matters**

Reinforcement learning competitions are amazing opportunities for new RL researchers to gain first-hand experience. MineRL offers a unique opportunity by providing human demonstration data. It is difficult for individual researchers to collect large amount of demonstrations to test their ideas. The competition alleviates this problem and allows researchers to implement their own algorithms without worrying about collecting data.

**Read more**

- [NeurIPS 2019: MineRL Competition (AICrowd)](https://www.aicrowd.com/challenges/neurips-2019-minerl-competition)
- [MineRL Project (Project Website)](http://minerl.io/index.html)
- [MineRL Minecraft Server (Project Website)](http://minerl.io/play/)
- [MineRL Dataset Description (Project Website)](http://minerl.io/dataset/)
- [MineRL Documentation (Read the Docs)](http://minerl.io/docs/)
- [The MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors (ArXiv Preprint)](https://arxiv.org/abs/1904.10079)

**External Resources**

- [2017 The Malmo Collaborative AI Challenge](https://www.microsoft.com/en-us/research/academic-program/collaborative-ai-challenge/)
- [2018 Learning to Play: the Mutli-agent Reinforcement Learning in MALMÖ (MARLÖ) Competition](https://marlo-ai.github.io/)






## Off-Policy Evaluation via Off-Policy Classification {#ope}

<p class="authors" style="font-size: 0.8em">
Alex Irpan<sup>1</sup>,
Kanishka Rao<sup>1</sup>,
Konstantinos Bousmalis<sup>2</sup>,
Chris Harris<sup>1</sup>,
Julian Ibarz<sup>1</sup>,
Sergey Levine<sup>13</sup>
</p>
<p class="authors__institutions" style="font-size: 0.8em">
    <sup>1</sup>Google Brain
    <sup>2</sup>DeepMind
    <sup>3</sup>UC Berkeley
</p>


<div class="w50" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/20/ope.png" alt="OPE">
</div>

**What it is & Why it matters**

Traditionally, a trained agent is evaluated by interacting with the target environment. Although this is feasible when the target environment is a simulated environment, it may be problematic in real-life applications like robotics. In these cases, off-policy evaluation (OPE) methods should be used. Different from existing OPE methods that require a good model of the environment or use importance sampling, this paper frames OPE as a "positive-unlabeled" classification problem. A state-action pair is labeled "effective" if an optimal policy can achieve success in that situation, and "catastrophic" otherwise. The intuition lies in that a well-learnt Q-function should return high value for effective state-action pair and low value for catastrophic state-action pair.

**Read more**

- [Off-Policy Evaluation via Off-Policy Classification (ArXiv Preprint)](https://arxiv.org/abs/1906.01624)
- [Code for the Binary Tree Environment (GitHub Gist)](https://gist.github.com/alexirpan/54ac855db7e0d017656645ef1475ac08)






## Towards Interpretable Reinforcement Learning Using Attention Augmented Agents {#attention}

<p class="authors" style="font-size: 0.8em">
Alex Mott<sup>*1</sup>,
Daniel Zoran<sup>*1</sup>,
Mike Chrzanowski<sup>1</sup>,
Daan Wierstra<sup>1</sup>,
Danilo J. Rezende<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 0.8em">
    <sup>1</sup>DeepMind
</p>


<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/20/attention.png" alt="">
</div>

**What it is & Why it matters**

The authors propose a LSTM architecture with a soft, top-down, spatial attention mechanism. The paper is not the first to propose using attention in RL agents, but the numerous experiments show how attention can be used to qualitatively evaluate and interpret agents' abilities. The project website below shows how attention can be used to understand how the agent reacts to novel states, how the agent plans, and what the agent's strategy is.

**Read more**

- [Towards Interpretable Reinforcement Learning Using Attention Augmented Agents (ArXiv Preprint)](https://arxiv.org/abs/1906.02500)
- [Towards Interpretable Reinforcement Learning Using Attention Augmented Agents (Project Website)](https://sites.google.com/view/s3ta)





---

Some more exciting news in RL:

- Researchers at Google Research open-sourced [Google Research Football](https://ai.googleblog.com/2019/06/introducing-google-research-football.html), an RL environment for football.
- Researchers at Electronic Arts and Institute of Computational Modelling trained Non-Player Characters (NPCs) in video games through [imitation learning with a human in the loop](https://arxiv.org/abs/1906.00535).
- Researchers at OpenAI performed an empirical study to show [the relationship between hyperparameters and generalization](https://arxiv.org/abs/1906.00431).
- Researchers at Nanjing University proposed [Clustered RL](https://arxiv.org/abs/1906.02457) that divides collected states into clusters and defines a clustering-based bonus reward to incentivise exploration.
- Researchers at MIT, Harvard, Diffeo, and CBMM developed [DeepRole](https://arxiv.org/abs/1906.02330), a multi-agent RL agent that learns who to cooperate with and outperforms humans in the game *The Resistance: Avalon*.
- Researchers at DeepMind and University of Toronto proposed [OPRE (OPtions as REsponses)](https://arxiv.org/abs/1906.01470), a multi-agent hierarchical agent.
- Researchers at University of Oxford propose [Independent Centrally-assisted Q-Learning (ICQL)](https://arxiv.org/abs/1906.02138) that allow using intrinsic rewards for multi-agent RL.
- Researchers at Microsoft Research Montreal and Imperial College London hypothesized that the poor performance of low discount factors are not due to small action-gaps but due to ["the size-difference of the action gaps across the state-space."](https://arxiv.org/abs/1906.00572)
