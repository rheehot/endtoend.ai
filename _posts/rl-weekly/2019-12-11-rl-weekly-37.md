---
layout: post
title: "RL Weekly 37: Observational Overfitting, Hindsight Credit Assignment, and Procedurally Generated Environment Suite"
author: Seungjae Ryan Lee
permalink: /rl-weekly/37
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/37/obs_overfit.png
image_type: contain
excerpt: "In this issue, we look at Google and MIT's study on the observational overfitting phenomenon and how overparametrization helps generalization, a new family of algorithms using hindsight credit assignment by DeepMind, and a new environment suite by OpenAI consisting of procedurally generated environments."

nav:
- name: "Observational Overfitting in Reinforcement Learning"
  permalink: "#observational-overfitting"
- name: "Hindsight Credit Assignment"
  permalink: "#hindsight-credit-assignment"
- name: "Leveraging Procedural Generation to Benchmark Reinforcement Learning"
  permalink: "#procgen"
- name: "More News"
  permalink: "#more-news"

related:
- title: "RL Weekly 36: AlphaZero with a Learned Model achieves SotA in Atari"
  link: /rl-weekly/36
  image: /assets/blog/rl-weekly/36/muzero.png
  image_type: contain
---


{% include revue.html %}

<style>
.letter, .letter p {
  color: gray;
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
.letter a {
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
</style>

<div class="letter">
<p>
Dear readers,
</p>
<p>
Happy NeurIPS! This week, I have made my summaries more concise to improve the reading experience. I hope that this change makes the newsletter easier to digest.
</p>
<p>
I wait for your feedback, either by <a href="mailto:seungjaeryanlee@gmail.com">email</a> or <a href="https://forms.gle/yZiHUXbtph8msVHn9">a feedback form</a>. Your input is always appreciated.
</p>
<p>
- Ryan
</p>
</div>



## Observational Overfitting in Reinforcement Learning {#observational-overfitting}

<p class="authors" style="font-size: 1em">
Xingyou Song<sup>1</sup>,
Yiding Jiang<sup>1</sup>,
Yilun Du<sup>2</sup>,
Behnam Neyshabur<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Google
    <sup>2</sup>MIT
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/37/obs_overfit.png" alt="">
</div>

<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/37/obs_overfit_diagram.png" alt="">
</div>


**What it says**

Observational overfitting is a phenomenon "where an agent overfits due to properties of the observation which are irrelevant to the latent dynamics of the MDP family." For example, in the saliency map above, the score and the background objects are highlighted red as they are deeply correlated with progress. This could hinder generalization: the authors report that simply covering the scoreboard with a black rectangle during training resulted in a 10% increased test performance. The authors use a Linear Quadratic Regulator (LQR) to validate the phenomenon, and find that overparametrizing potentially helps as a form of "implicit regularization." The authors also try ImageNet networks (AlexNet, Inception, ResNet, etc.) on CoinRun environments, and show that overparametrization improves generalization to the test set.

**Read more**

- [arXiv Preprint](https://arxiv.org/abs/1912.02975)
- [ICLR2020 OpenReview](https://openreview.net/forum?id=HJli2hNKDH)










## Hindsight Credit Assignment {#hindsight-credit-assignment}

<p class="authors" style="font-size: 1em">
Anna Harutyunyan<sup>1</sup>,
Will Dabney<sup>1</sup>,
Thomas Mesnard<sup>1</sup>,
Nicolas Heess<sup>1</sup>,
Mohammad G. Azar<sup>1</sup>,
Bilal Piot<sup>1</sup>,
Hado van Hasselt<sup>1</sup>,
Satinder Singh<sup>1</sup>,
Greg Wayne<sup>1</sup>,
Doina Precup<sup>1</sup>,
RÃ©mi Munos<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>DeepMind
</p>

<!-- <div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/37/todo.png" alt="">
</div> -->

**What it says**

Estimating the value function is a critical part of RL, as it quantifies how choosing an action in a state affects future return. The reverse of this is the credit assignment question: "given an outcome, how relevant were past decisions?" The authors define the "hindsight distribution" of an action as the conditional probability of the first action of the trajectory being that action over trajectories given some outcome (either state-dconditional or return-conditional). This learned hindsight distribution can be used to better estimate value functions or policy gradients. The authors validate new algorithms that use Hindsight Credit Assignment in a few diagnostic tasks.

**Read more**

- [arXiv Preprint](https://arxiv.org/abs/1912.02503)
- [Twitter Tweet](https://twitter.com/aharutyu/status/1202594577842085888)
- [NeurIPS 2019 Poster](https://twitter.com/aharutyu/status/1202594577842085888/photo/1)









## Leveraging Procedural Generation to Benchmark Reinforcement Learning {#procgen}

<p class="authors" style="font-size: 1em">
Karl Cobbe<sup>1</sup>,
Christopher Hesse<sup>1</sup>,
Jacob Hilton<sup>1</sup>,
John Schulman<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>OpenAI
</p>

<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/37/procgen.png" alt="">
</div>

**What it says**

OpenAI (Cobbe et al.) released a set of 15 new environments similar to the CoinRun environment released last year, where the environments are "procedurally generated." Having content procedurally generated in many aspects (level layout, game assets, entity spawn location and timing, etc.) encourages the agent to learn a policy robust to such variations. Procedurally generated environments also allow for a natural division of training and test set by generating different environments.

**Read more**

- [arXiv Preprint](https://arxiv.org/abs/1912.01588)
- [GitHub Repository](https://github.com/openai/procgen)

**External resources**

- [Quantifying Generalization in Reinforcement Learning (arXiv Preprint)](https://arxiv.org/abs/1812.02341): The original paper for CoinRun
- [Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning](https://arxiv.org/abs/1902.01378): Obstacle Tower, a 3D procedurally generated environment by Unity

















------

<div id="more-news"></div>

Here are some more exciting news in RL:

[**Reinforcement Learning: Past, Present, and Future Perspectives**](https://slideslive.com/38921493/reinforcement-learning-past-present-and-future-perspectives)
<br/>
The recording of a NeurIPS 2019 presentation on RL by Katja Hofmann (Microsoft Research) is available online.

[**Stable Baselines - Reinforcement Learning Tips and Tricks**](https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html)
<br/>
Stable Baselines, a major well-maintained fork of OpenAI Baselines, released a set of tips and tricks for RL.

[**Winner Announced for NeurIPS 2019: Learn to Move - Walk Around**](https://www.aicrowd.com/challenges/neurips-2019-learn-to-move-walk-around)
<br/>
The winners for each track of the NeurIPS 2019: Learn to Move - Walk Around was announced.

[**New State-of-the-art for Hanabi**](https://ai.facebook.com/blog/building-ai-that-can-master-complex-cooperative-games-with-hidden-information/)
<br/>
Facebook AI wrote a blog post on how they build a new bot that achieves state-of-the-art in Hanabi, a collaborative card game.

<!-- [**Behavior Regularized Offline Reinforcement Learning**](https://arxiv.org/abs/1911.11361)
<br/>
TODO -->

<!-- [**Combining Q-Learning and Search with Amortized Value Estimates**](https://arxiv.org/abs/1912.02807)
<br/>
TODO -->

<!-- [**AlgaeDICE: Policy Gradient from Arbitrary Experience**](https://arxiv.org/abs/1912.02074)
<br/>
TODO -->

<!-- [**TODO**](todo)
<br/>
TODO -->



