---
layout: post
title: "RL Weekly 29: The Behaviors and Superstitions of RL, and How Deep RL Compares with the Best Humans in Atari"
author: Seungjae Ryan Lee
permalink: /rl-weekly/29
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/29/bsuite_radar_plot.png
image_type: contain
excerpt: "In this issue, we look at reinforcement learning from a wider perspective. We look at new environments and experiments that are designed to test and challenge the agents' capabilities. We also compare existing RL agents against the playthroughs of best Atari human players."

nav:
- name: "Behavior Suite for RL"
  permalink: "#bsuite"
- name: "Superstition in the Network"
  permalink: "#superstition"
- name: "Deep RL superhuman on Atari?"
  permalink: "#saber"

related:
- title: "RL Weekly 28: Free-Lunch Saliency and Hierarchical RL with Behavior Cloning"
  link: /rl-weekly/28
  image: /assets/blog/rl-weekly/28/fls.png
  image_type: contain
- title: "RL Weekly 30: Learning State and Action Embeddings, a New Framework for RL in Games, and an Interactive Variant of Question Answering"
  link: /rl-weekly/30
  image: /assets/blog/rl-weekly/30/dyne.png
  image_type: contain
---


{% include revue.html %}


## Behavior Suite for RL {#bsuite}

<p class="authors" style="font-size: 1em">
Dmitry Nikulin<sup>1</sup>,
Ian Osband<sup>1</sup>,
Yotam Doron<sup>1</sup>,
Matteo Hessel<sup>1</sup>,
John Aslanides<sup>1</sup>,
Eren Sezener<sup>1</sup>,
Andre Saraiva<sup>1</sup>,
Katrina McKinney<sup>1</sup>,
Tor Lattimore<sup>1</sup>,
Csaba Szepezvari<sup>1</sup>,
Satinder Singh<sup>1</sup>,
Benjamin Van Roy<sup>1</sup>,
Richard Sutton<sup>1</sup>,
David Silver<sup>1</sup>,
Hado Van Hasselt<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>DeepMind
</p>

<div class="w50">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/29/bsuite_radar_plot.png" alt="">
</div>

**What it says**

A "good" RL agent might not be the agent that simply has the best score on a certain environment. An efficient RL algorithm must handle the various challenges the RL framework gives:

1. Generalization from collected data
2. Eexploration-exploitation tradeoff
3. Long-term planning

It is difficult to assess these features of an agent in a separated setting. For this reason, the authors created bsuite, a collection of diagnostic experiments to analyze the capabilities of RL agents and provide insight.

In bsuite, Each environment is designed to measure the agent's capabilities on a certain issue. It is designed to be challenging enough to push agents beyond their capabilities, while being simple enough to be able to focus on key issues. As a benchmark, these environments are also fast and scalable.

The authors emphasize that bsuite will be continuously developed, with new experiments added from the research community.

**Read more**

- [Behaviour Suite for Reinforcement Learning (ArXiv Preprint)](https://arxiv.org/abs/1908.03568)
- [bsuite - Behaviour Suite for Reinforcement Learning (GitHub Repo)](http://github.com/deepmind/bsuite)




## “Superstition” in the Network {#superstition}

<p class="authors" style="font-size: 1em">
Philip Bontrager<sup>1</sup>,
Ahmed Khalifa<sup>1</sup>,
Damien Anderson<sup>2</sup>,
Matthew Stephenson<sup>3</sup>,
Christoph Salge<sup>4</sup>,
Julian Togelius<sup>1</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>New York University,
    <sup>2</sup>University of Strathclyde,
    <sup>3</sup>Maastricht University,
    <sup>4</sup>University of Hertfordshire
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/29/superstition_envs.png" alt="">
</div>
<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/29/superstition_table.png" alt="">
</div>

**What it says**

Deep RL agents have been successful in a lot of games, yet it still fails on many games. To characterize the environments that "trick" the agent well, the authors present four "deceptive" games, each with a different type of deception.

The first is DeceptiCoins (Top left). The agent can choose between two paths: left and right. The left path has an earlier but smaller reward, and the right path has delayed but greater reward. This is the classical exploration vs. exploitation problem.

The second is WaferThinMints (Top right), the "Subverted Generalization" problem. The agent is rewarded for collecting mints, but after collecting 9 mints, the agent is penalized for collecting mints. The agent can check the number of mints collected through a small green bar that follows the agent. The agent is deceived to conclude that collecting mints are good, but the expectation is betrayed on the 10th mint.

The third is Flower (Bottom left), the "Delayed Reward" problem. The agent supervises a field with flower seeds that grow into flowers, and the agent is given higher reward for flowers than seeds. The optimal agent should wait until the flower is fully grown, then harvest it quickly.

The fourth is Invest (Bottom right), the "Delayed Gratification" problem. The agent can invest their reward, incurring a penalty, to receive greater reward after some time.

As shown in the table above, the A2C agent fails to learn optimal behavior in many environments.

(The authors examine in great detail how an A2C agent and various planning agents act throughout training. Try formulating a hypothesis about each learning curve, and read their explanation to see if you are correct!)

**Read more**

- ["Superstition" in the Network: Deep Reinforcement Learning Plays Deceptive Games (ArXiv Preprint)](https://arxiv.org/abs/1908.04436)

**External resources**

- [The General Video Game AI Competition](http://www.gvgai.net/)




## Is Deep RL Really Superhuman on Atari? {#saber}

<p class="authors" style="font-size: 1em">
Marin Toromanoff<sup>1</sup>,
Emilie Wirbel<sup>2</sup>,
Fabien Moutarde<sup>3</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>MINES ParisTech, Valeo DAR, Valeo.ai,
    <sup>2</sup>Valeo Driving Assistance Research,
    <sup>3</sup>Center for Robotics, MINES ParisTech, PSL
</p>

<div class="w40">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/29/saber_beginner_vs_pro.png" alt="">
</div>

**What it says**

Since Deep Q-Networks (DQN) and its various improvements (namely Rainbow and IQN), the Atari environments provided via the Arcade Learning Environment (ALE) has been labeled as "solved," with the agents performing in "superhuman" levels in most. The authors propose a benchmark SABER (Standardized Atari BEnchmark for RL) following the advice of Machado et al. about evaluations in ALE (2017). The recommendations are (1) adding stochasticity through "sticky actions," (2) ignoring the loss of life signal, (3) using the full action space of 18, and (4) averaging performances over 100 episodes.

The authors also argue that following other RL works such as AlphaGo, AlphaStar, and OpenAI Five, the Atari agents should be compared against the world's best humans. Thus, the authors compile a human world records baseline. With Rainbow and their own Rainbow-IQN, they show that RL agents only outperform the best human scores in only 3 or 4 games.

**Read more**

- [Is Deep Reinforcement Learning Really Superhuman on Atari? (ArXiv Preprint)](https://arxiv.org/abs/1908.04683)

**External resources**

- [Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents (ArXiv Preprint)](https://arxiv.org/abs/1709.06009)




------

<div id="one-liners"></div>

One-line introductions to more exciting news in RL this week:

- [**Review of Cooperative MARL**](https://arxiv.org/abs/1908.03963): A review of cooperative multi-agent deep RL.
- [**Review on Deep RL for Fluid Mechanics**](https://arxiv.org/abs/1908.04127): What deep RL algorithms have been used in fluid mechanics?
- [**Reward Function Tampering**](https://arxiv.org/abs/1908.04734): How can we assure that the agent reward corresponds to user utility? In other words, how can we prevent agents achieving high rewards without performing wanted tasks?
- [**Manipulation via Locomotion**](https://arxiv.org/abs/1908.05224): Train hierarchical RL where low-level locomotive skills are used for high-level object manipulation. This policy can then be transferred to the real world with zero real world training.
