---
layout: post
title: "RL Weekly 11: The Bitter Lesson by Richard Sutton, the Promise of Hierarchical RL, and Exploration with Human Feedback"
author: Seungjae Ryan Lee
permalink: /rl-weekly/11
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/11/combination.png
image_type: contain
excerpt: "In this issue, we first look at a diary entry by Richard S. Sutton (DeepMind, UAlberta) on Compute versus Clever. Then, we look at a post summarizing Hierarchical RL by Yannis Flet-Berliac (INRIA SequeL). Finally, we summarize a paper incorporating human feedback for exploration from Delft University of Technology."

nav:
- name: "The Bitter Lesson"
  permalink: "#the-bitter-lesson"
- name: "The Promise of HRL"
  permalink: "#the-promise-of-hierarchical-reinforcement-learning"
- name: "Exploration with Human Feedback"
  permalink: "#exploration-with-human-feedback"

related:
- title: "RL Weekly 10: Learning from Playing, Understanding Multi-agent Intelligence, and Navigating in Google Street View"
  link: /rl-weekly/10
  image: /assets/blog/rl-weekly/10/lfp.png
  image_type: contain
---


## The Bitter Lesson

**What it is**

Richard S. Sutton, one of the founding fathers of modern reinforcement learning, wrote a new blog post titled "The Bitter Lesson." In the post, he argues that
"general methods that leverage computation are ultimately the most effective." Because the computational resources increase as time passes (Moore's Law), methods that withstand the challenge of time are ones that scales well and can utilize massive amounts of computation.

Thus, Sutton argues that researchers should be wary of seeking to leverage human knowledge into their research, as it often runs counter to leveraging computation. To further illustrate this, Sutton gives Chess, Go, speech recognition and computer vision as examples. Handcrafted evaluation methods and features were popular on early stages of AI in each of these fields, but today's state of the art methods with superhuman results have discarded such complex methods. "We want AI agents that can discover like we can, not which contain what we have discovered."

**Why it matters**

Reinforcement learning is a blooming field: dozens of labs publish new papers every week, claiming an improvement against previous papers. Many of these papers infuse human priors into their methods. This blog post is a nice reminder to researchers choosing research ideas and readers assessing the impact of research.
 
Renowned researchers also shared their own thoughts about the blog post, which provide further insight of the topic.

**Read more**

- [The Bitter Lesson (Blog Post)](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

**External Resources**

- [David Ha's Response (Twitter Thread)](https://twitter.com/hardmaru/status/1107158803957469184)
- [Kyunghyun Cho's Response (Twitter Thread)](https://twitter.com/kchonyc/status/1106893894560768001)
- [Shimon Whiteson's Response (Twitter Thread)](https://twitter.com/shimon8282/status/1106534178676506624)
- [The Bitter Lesson (ML Subreddit)](https://www.reddit.com/r/MachineLearning/comments/b179cs/d_the_bitter_lesson/)
- ["The Bitter Lesson": Compute Beats Clever (RL Subreddit)](https://www.reddit.com/r/reinforcementlearning/comments/b16pd4/the_bitter_lesson_compute_beats_clever_rich/)
- [The Bitter Lesson (Hacker News)](https://news.ycombinator.com/item?id=19393432)



## The Promise of Hierarchical Reinforcement Learning

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/11/beef.png" alt="">
</div>

**What it is**

This post by Flet-Berliac summarizes hierarchical RL (HRL) and introduces recent advances in HRL. Hierchical RL uses a hierarchical structure to break down large problem into multiple small problems, allowing agents to solve problems to complex with "flat" RL. Existing HRL algorithms suffer from several defects, performing worse than "flat" RL algorithms in most settings. Still, HRL shows great promise in specific domains with greater sample efficiency and better generalization.

**Read more**

- [The Promise of Hierarchical Reinforcement Learning (The Gradient Story)](https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/)

**External Resources**

- [FeUdal Networks for Hierarchical Reinforcement Learning (ArXiv Preprint)](https://arxiv.org/abs/1703.01161)
- [The Option-Critic Architecture (ArXiv Preprint)](https://arxiv.org/abs/1609.05140)
- [Data-Efficient Hierarchical Reinforcement Learning (Arxiv Preprint)](https://arxiv.org/abs/1805.08296)
- [Learning Multi-Level Hierarchies with Hindsight (ArXiv Preprint)](https://arxiv.org/abs/1712.00948)
- [Learning and Transfer of Modulated Locomotor Controllers (ArXiv Preprint)](https://arxiv.org/abs/1610.05182)
- [On Reinforcement Learning for Full-length Game of StarCraft (ArXiv Preprint)](https://arxiv.org/abs/1809.09095)
- [Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation (ArXiv Preprint)](https://arxiv.org/abs/1604.06057)
- [Meta Learning Shared Hierarchies (ArXiv Preprint)](https://arxiv.org/abs/1710.09767)
- [Strategic Attentive Writer for Learning Macro-Actions (ArXiv Preprint)](https://arxiv.org/abs/1606.04695)
- [A Deep Hierarchical Approach to Lifelong Learning in Minecraft (ArXiv Preprint)](https://arxiv.org/abs/1604.07255)
- [Planning with Abstract Markov Decision Processes (AAAI Publications)](https://aaai.org/ocs/index.php/ICAPS/ICAPS17/paper/view/15759)
- [Iterative Hierarchical Optimization for Misspecified Problems (IHOMP) (ArXiv Preprint)](https://arxiv.org/abs/1602.03348)
- [Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement Learning (ArXiv Preprint)](https://arxiv.org/abs/1811.09083)
- [Learning Representations in Model-Free Hierarchical Reinforcement Learning (ArXiv Preprint)](https://arxiv.org/abs/1810.10096)


## Exploration with Human Feedback

<div class="w80" style="margin: 10px auto;">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/11/combination.png" alt="">
</div>

**What it is**

Researchers at Delft University of Technology developed Probabilistic Merging of Policies (PPMP) as a method to incorporate binary corrective feedback to actor critic algorithms to improve sample efficiency and performance. Environments with simple action space (Pendulum, Mountain Car, and Lunar Lander) is used, and human feedback is selected from three numbers: -1, 0, 1. When the actor's action deviate a lot from optimal action, PPMP facilitates exploration aggressively, and when the selected action is near optimal, PPMP adds little stochasticity to the action. This allows for a natural curriculum learning setup, and allows the fully trained agent to run independently without human feedback.

**Why it matters**

Distilling human knowledge can be powerful to the efficiency, performance, and robustness of ML models and RL agents. Imitation learning can work well, but it requires precise trajectories that are hard to obtain. In environments where the action space is simple and optimal action is clear, such feedback-based methods could boost the agent considerably.

**Read more**

- [Deep Reinforcement Learning with Feedback-based Exploration (ArXiv Preprint)](https://arxiv.org/abs/1903.06151)
- [Predictive Probabilistic Merging of Policies (GitHub Repo)](https://github.com/janscholten/ppmp)

---

Some more exciting news in RL:

- DeepMind published a new post on AI safety named ["Designing agent incentives to avoid side effects"](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-side-effects-e1ac80ea6107).
- [MicroRTS competition](https://sites.google.com/site/micrortsaicompetition/) will occur at IEEE Conference on Games (CoG) 2019.
