---
published: false
layout: post
title: "RL Weekly 40: TODO"
author: Seungjae Ryan Lee
permalink: /rl-weekly/40
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/40/todo.png
image_type: contain
excerpt: "In this issue, we look at TODO"

nav:
- name: "TODO"
  permalink: "#todo"
- name: "TODO"
  permalink: "#todo"
- name: "TODO"
  permalink: "#todo"
- name: "More News"
  permalink: "#more-news"

related:
- title: "RL Weekly 39: Intrinsic Motivation for Cooperation and Amortized Q-Learning"
  link: /rl-weekly/39
  image: /assets/blog/rl-weekly/39/synergistic_overview.png
  image_type: contain
---


{% include revue.html %}

<style>
.letter, .letter p {
  color: gray;
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
.letter a {
  font-family: "Helvetica", "Arial", sans-serif;
  font-size: 16px;
  font-style: italic;
  font-weight: 400;
  line-height: 20px;
}
</style>

<div class="letter">
<p>
Dear readers,
</p>
<p>
I am glad to tell you that RL Weekly reached over 1000 subscribers! When I was writing my first issue, I did not imagine that 1000 people would read what I wrote! I will continue to collect exciting news in RL and summarize them to the best of my abilities. I hope you will stay with me in this wonderful journey!
</p>
<p>
In this issue, we look at TODO.
</p>
<p>
I would love to hear your feedback! If you have anything to say, please <a href="mailto:seungjaeryanlee@gmail.com">email me</a>.
</p>
<p>
- Ryan
</p>
</div>



## TODO {#todo}

<p class="authors" style="font-size: 1em">
Author<sup>1</sup>,
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Institution
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/40/todo.png" alt="">
</div>

**What it says**

**Read more**

**External resources**






## TODO {#todo}

<p class="authors" style="font-size: 1em">
Author<sup>1</sup>,
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Institution
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/40/todo.png" alt="">
</div>

**What it says**

**Read more**

**External resources**






## TODO {#todo}

<p class="authors" style="font-size: 1em">
Author<sup>1</sup>,
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Institution
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/40/todo.png" alt="">
</div>

**What it says**

**Read more**

**External resources**



------

<div id="more-news"></div>

Here is some more exciting news in RL:

<!-- [**TODO**](todo)
<br/>
TODO -->

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**TODO**](todo)
<br/>
TODO

[**Self-Tuning Deep Reinforcement Learning**](https://arxiv.org/abs/2002.12928)
<br/>
Use metagradients to self-tune important hyperparameters of IMPALA such as discount factor, bootstrapping parameter, and loss weights, resulting in better performance.

[**Jelly Bean World: A Testbed for Never-Ending Learning**](https://arxiv.org/abs/2002.06306)
<br/>
A new 2D gridworld environment for lifelong learning with vision and "scent" observations.



## Looks Interesting

- [Optimistic Exploration even with a Pessimistic Initialisation](https://arxiv.org/abs/2002.12174)
- [On Catastrophic Interference in Atari 2600 Games](https://arxiv.org/abs/2002.12499)
  - https://twitter.com/its_dibya/status/1234516717268762624
  - https://twitter.com/its_dibya/status/1234517494007762944
- [Generalized Hindsight for Reinforcement Learning](https://arxiv.org/abs/2002.11708)
  - https://twitter.com/pabbeel/status/1234238582832787456
- [Policy Evaluation Networks](https://arxiv.org/abs/2002.11833)
- [Estimating Q(s,s') with Deep Deterministic Dynamics Gradients](https://arxiv.org/abs/2002.09505)
  - https://twitter.com/RealAshEdwards/status/1232121559256428544
- [Maxmin Q-learning: Controlling the Estimation Bias of Q-learning](https://arxiv.org/abs/2002.06487)
- https://twitter.com/katjahofmann/status/1231988256532836354

<!-- - [Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration](https://arxiv.org/abs/2002.09253) -->
<!-- - [How Transferable are the Representations Learned by Deep Q Agents?](https://arxiv.org/abs/2002.10021) -->
<!-- - [RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments](https://arxiv.org/abs/2002.12292) -->

<!-- - [How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents](https://arxiv.org/abs/2002.08795) -->
<!-- - [REST: Performance Improvement of a Black Box Model via RL-based Spatial Transformation](https://arxiv.org/abs/2002.06610) -->
<!-- - [Review, Analyze, and Design a Comprehensive Deep Reinforcement Learning Framework](https://arxiv.org/abs/2002.11883) -->
<!-- - [TanksWorld: A Multi-Agent Environment for AI Safety Research](https://arxiv.org/abs/2002.11174) -->
<!-- - [Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement](https://arxiv.org/abs/2002.11089) -->
<!-- - [Discriminative Particle Filter Reinforcement Learning for Complex Partial Observations](https://arxiv.org/abs/2002.09884) -->
<!-- - [ConQUR: Mitigating Delusional Bias in Deep Q-learning](https://arxiv.org/abs/2002.12399) -->


