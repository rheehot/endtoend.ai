---
layout: post
title: "RL Weekly 27: Diverse Trajectory-conditioned Self Imitation Learning and Environment Probing Interaction Policies"
author: Seungjae Ryan Lee
permalink: /rl-weekly/27
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/27/dt-sil.png
image_type: contain
excerpt: "This week, we look at a self imitation learning method that imitates diverse past experience for better exploration. We also summarize an environment probing policy that helps an agent adapt to different environments."

nav:
- name: "Diverse Trajectory-conditioned Self Imitation Learning"
  permalink: "#dt-sil"
- name: "Environment Probing Interaction Policies"
  permalink: "#epi"
- name: "One-liners"
  permalink: "#one-liners"

related:
- title: "RL Weekly 26: Transfer RL with Credit Assignment and Convolutional Reservoir Computing for World Models"
  link: /rl-weekly/26
  image: /assets/blog/rl-weekly/26/rcrc.png
  image_type: contain
---


{% include revue.html %}


## Diverse Trajectory-conditioned Self Imitation Learning {#dt-sil}

<p class="authors" style="font-size: 1em">
Yijie Guo<sup>1</sup>,
Jongwook Choi<sup>1</sup>,
Marcin Moczulski<sup>2</sup>,
Samy Bengio<sup>2</sup>,
Mohammad Norouzi<sup>2</sup>,
Honglak Lee<sup>21</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>University of Michigan
    <sup>2</sup>Google Brain
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/27/dt-sil.png" alt="">
</div>

**What it says**

Self-imitation learning is a method of imitating one's own trajectories to improve learning efficiency. In past work, only the good trajectories were selected, but this could mislead the agent to settle for local optima, as high return in the short term does not conclude high return in the long term.

Thus, the authors propose using a buffer to store all past trajectories, and imitate any trajectory in this buffer, "exploiting the past experience in diverse directions." Each element in the buffer contains the trajectory, the embedding of the final state of the trajectory, and the visitation count of this embedded final state (Section 3.2). It is assumed that such embedding is provided (Section 3.1).

For every timestep, the embedded final state is computed. If such embedding has never been seen, a new element is added to the buffer. If there is another element in the buffer with the same embedding, the visitation count is incremented, and the "better" trajectory leading to the embedded final state is kept in the buffer.

To learn the policy, the demonstration trajectories are sampled stochastically, with the weights being inverse proportional to square root of visitation count. Thus, trajectories with less-visited final states are prioritized for imitation. (Section 3.3).

The trajectory-conditioned policy is represented by an RNN with attention mechanism (Section 3.4), where given a demonstration trajectory and current partial trajectory, the policy predicts the next action to imitate the demonstration. This is trained with RL through policy gradients by defining imitation rewards, and it is also trained with supervised learning by using same trajectories for both demonstration and current policy and predicting the  action selected in the demonstration.

This algorithm DT-SIL is tested against PPO, PPO with count-based exploration bonus, and PPO with Self-Imitation Learning. DT-SIL shows superior performance in various domains, and achieves the score of 29278 in Atari Montezuma's Revenge.


**Read more**

- [Efficient Exploration with Self-Imitation Learning via Trajectory-Conditioned Policy (ArXiv Preprint)](https://arxiv.org/abs/1907.10247)







## Environment Probing Interaction Policies {#epi}

<p class="authors" style="font-size: 1em">
Wenxuan Zhou<sup>1</sup>,
Lerrel Pinto<sup>1</sup>,
Abhinav Gupta<sup>12</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>The Robotics Institute, Carnegie Mellon University
    <sup>2</sup>Facebook AI Research
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/27/epi.png" alt="">
</div>

**What it says**

Although RL has achieved great success in many fields, it has the problem of overfitting to the environment, failing to generalize to the new environment. One way to mitigate this issue is by training the policy in multiple environments, but this often leads to a conservative policy that does not exploit environment dynamics.

Another way to mitigate this issue is to design a policy that adapts to the environment. Since identifying the environment is infeasible and challenging, the authors propose "Environment Probing Interaction" (EPI), where the agent also learns an EPI policy separate from the task-specific policy.

The EPI policy is trained using two next-state-prediction networks (Figure 1). One is a simple prediction network, and another is a network that is given extra information from the trajectory embedding network that is also being trained. EPI policy is rewarded by how much better the prediction of the latter network is, so it is encouraged to generate trajectories that can be embedded to gain useful information about the environment. (Section 4.1).

The EPI policy is tested n two MuJoCo environments: Striker and Hopper. The environment is altered by changing various physical properties such as the body mass, the joint dampings, and the friction coefficient. The EPI policy shows performance superior to the simple MLP policy and "invariant policy" (Section 5).


**Read more**

- [Environment Probing Interaction Policies (ArXiv Preprint)](https://arxiv.org/abs/1907.11740)
- [Environment Probing Interaction Policies (OpenReview)](https://openreview.net/forum?id=ryl8-3AcFX)
- [Environment Probing Interaction Policies (GitHub Repo)](https://github.com/Wenxuan-Zhou/EPI.git)



------

<div id="one-liners"></div>

One-line introductions to more exciting news in RL this week:

- **Environments**
  - [**Psycholab**](https://github.com/google-research/google-research/tree/master/psycholab): Create a multi-agent gridworld game with an ASCII representation!
  - [**CuLE**](https://arxiv.org/abs/1907.08467): Run thousands of Atari environments simultaneously with GPU's superior parallelization ability!
  - [**Arena**](https://arxiv.org/abs/1907.09467): A multi-agent RL research tool supporting popular environments such as StarCraft II, Pommerman, VizDoom, and Soccer!
- **Competitions**
  - The Unity Obstacle Tower Challenge has come to an end! Here are two writeups from the participants:
    - [1st Place](https://blog.aqnichol.com/2019/07/24/competing-in-the-obstacle-tower-challenge/)
    - [4th Place: PPO Dash](https://towardsdatascience.com/i-placed-4th-in-my-first-ai-competition-takeaways-from-the-unity-obstacle-tower-competition-794d3e6d3310#4167-33be8d61bdc2)
  - [**MineRL**](https://arxiv.org/abs/1907.13440) The whitepaper for the MineRL NeurIPS competition was released!
- **Algorithms & Theories**
  - [**A3C TP**](https://arxiv.org/abs/1907.10827): Predicting closeness to episode termination while training A3C can boost its performance!
  - [**Hindsight TRPO**](https://arxiv.org/abs/1907.12439): Use hindsight with TRPO to outperform Hindsight Policy Gradient (HPG) in various environments!
  - [**DS-VIC**](https://arxiv.org/abs/1907.10580): Identify decision states in a goal-independent manner!
- **Miscellaneous**
  - [**Coursera Specialization**](https://www.coursera.org/specializations/reinforcement-learning) There is a new Coursera specialization on RL from University of Alberta!
