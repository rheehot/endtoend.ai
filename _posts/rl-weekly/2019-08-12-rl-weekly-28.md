---
layout: post
title: "RL Weekly 28: Free-Lunch Saliency and Hierarchical RL with Behavior Cloning"
author: Seungjae Ryan Lee
permalink: /rl-weekly/28
tags:
 - reinforcement-learning
 - rl-weekly

image: /assets/blog/rl-weekly/28/fls.png
image_type: contain
excerpt: "This week, we first look at Free-Lunch Saliency, a built-in interpretability module that does not deteriorate performance. Then, we look at HRL-BC, a combination of high-level RL policy with low-level skills trained through behavior cloning."

nav:
- name: "Free-Lunch Saliency"
  permalink: "#fls"
- name: "Hierarchical RL with Behavior Cloning"
  permalink: "#hrl-bc"
- name: "One-liners"
  permalink: "#one-liners"

related:
- title: "RL Weekly 27: Diverse Trajectory-conditioned Self Imitation Learning and Environment Probing Interaction Policies"
  link: /rl-weekly/27
  image: /assets/blog/rl-weekly/27/dt-sil.png
  image_type: contain
- title: "RL Weekly 29: The Behaviors and Superstitions of RL, and How Deep RL Compares with the Best Humans"
  link: /rl-weekly/29
  image: /assets/blog/rl-weekly/29/bsuite_radar_plot.png
  image_type: contain
---


{% include revue.html %}


## Free-Lunch Saliency via Attention in Atari Agents {#fls}

<p class="authors" style="font-size: 1em">
Dmitry Nikulin<sup>1</sup>,
Anastasia Ianina<sup>1</sup>,
Vladimir Aliev<sup>1</sup>,
Sergey Nikolenko<sup>123</sup>

</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Samsung AI Center, Moscow, Russia
    <sup>2</sup>Steklov Institute of Mathematics at St. Petersburg, Russia
    <sup>3</sup>Neuromation OU, Tallinn, Estonia
</p>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/28/fls.png" alt="">
</div>

<div class="w80">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/28/adhoc_saliency.png" alt="">
</div>

<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/28/architecture.png" alt="">
</div>

**What it says**

To interpret Deep RL agents, saliency maps are commonly used to highlight pixel areas that the agent deemed important. There are two ways to generate such map: post-hoc saliency method and built-in saliency method. Post-hoc saliency method reserves interpreting the agent after the training is complete, whereas built-in saliency methods use specific models that improve interpretability (Section 2). This work focuses on the built-in methods.

As shown in Table 1 above, there has been multiple works on built-in methods, but these methods have worse performance compared to their non-interpretable versions. The authors propose a new method named Free-Lunch Saliency, claiming that the interpretability comes "free" without the performance drop. The FLS module is situated between the convolutional layers and the fully connected layers.

For the performance, the authors run experiments on 6 Atari environments and verify that Sparse FLS does not lead to a performance drop (Section 4.2, Table 2). The authors show that having smaller receptive fields and strides allows for crisper saliency maps (Dense FLS). However, it also increases more memory and necessitates sum-pooling. The authors report this leads to a significantly worse performance.

For the quality of interpretability, the authors also compare the saliency maps generated by various built-in methods using the Atari-HEAD human dataset. The authors note that although no method clearly dominates others, they all perform better than random (Section 4.3).

**Read more**

- [Free-Lunch Saliency via Attention in Atari Agents (ArXiv Preprint)](https://arxiv.org/abs/1908.02511)
- [Free-Lunch Saliency via Attention in Atari Agents (GitHub Repo)](https://github.com/dniku/free-lunch-saliency)
- [Free-Lunch Saliency: Breakout, MsPacman, SpaceInvaders, Enduro, Seaquest (YouTube Video)](https://www.youtube.com/watch?v=i41rQXKsa50)

**External resources**

- [Learn to Interpret Atari Agents (ArXiv Preprint)](https://arxiv.org/abs/1812.11276): Region Sensitive Rainbow (RS-Rainbow)
- [Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset (ArXiv Preprint)](https://arxiv.org/abs/1903.06754)







## Hierarchical RL with Behavior Cloning {#hrl-bc}

<p class="authors" style="font-size: 1em">
Robin Strudel<sup>*1</sup>,
Alexander Pashevich<sup>*2</sup>,
Igor Kalevatykh<sup>1</sup>,
Ivan Laptev<sup>1</sup>,
Josef Sivic<sup>1</sup>,
Cordelia Schmid<sup>2</sup>
</p>
<p class="authors__institutions" style="font-size: 1em">
    <sup>1</sup>Inria, École normale supérieure, CNRS, PSL Research University, 75005 Paris, France
    <sup>2</sup>University Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.
</p>

<div class="w90">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/28/hrl-bc.png" alt="">
</div>

<div class="w60">
  <img src="{{ absolute_url }}/assets/blog/rl-weekly/28/ur5.png" alt="">
</div>

**What it says**

Imitation learning methods learn to solve a task through demonstrations. Although it is efficient for learning short trajectories with limited variability, the agent struggles when it observes states not seen in the demonstration. Reinforcement learning is able to learn without such demonstration dataset and generalizes better. However, it is not sample efficient since it relies on exploration to generalize.

To get the best of both worlds, the authors propose HRL-BC, a method of combining imitation learning and reinforcement learning through hierarchical RL (HRL). First, the agent learns primitive skill policies through behavior cloning (BC) (Section 3.A). Afterwards, the agent trains a master policy through Proximal Policy Optimization (PPO) that selects actions at a slower rate (Section 3.B).

The authors test their HRL-BC approach on both simulated and real robot arms (UR5). The authors report that the agent was able to learn skills quickly through behavior cloning with ResNet architecture and data augmentation (Section 4). Furthermore, HRL-BC shows superior success rate to methods that use only imitation learning or reinforcement learning (Section 5.B).


**Read more**

- [Combining learned skills and reinforcement learning for robotic manipulations (ArXiv Preprint)](https://arxiv.org/abs/1908.00722)


------

<div id="one-liners"></div>

One-line introductions to more exciting news in RL this week:

- [**Deep RL in System Optimization**](https://arxiv.org/abs/1908.01275) A paper studying the efficacy of deep RL methods in various system optimization problems!
- [**DoorGym**](https://arxiv.org/abs/1908.01887) A new robot control environment, where the agent needs to open a door!
- [**Weight Agnostic NN**](https://github.com/google/brain-tokyo-workshop/tree/master/WANNRelease) The code for Weight Agnostic Neural Network (WANN) was released on GitHub!
- [**Learn to Move: Walk Around**](https://www.aicrowd.com/challenges/neurips-2019-learning-to-move-walk-around) This NeurIPS 2019 competition offers $200 Google Cloud credit to first 200 submissions!
- [**Genetically Generated Macro Actions**](https://arxiv.org/abs/1908.01478): Make macro action construction independent by using genetic algorithms instead of past policy!
