---
layout: post
title: "Current Topics in Reinforcement Learning: Neural Fitted Q Iteration (Riedmiller, 2005)"
author: Seungjae Ryan Lee
permalink: /ctrl/neural-fitted-q-iteration/
redirect-from: /ctrl/nfq/
published: false

front_image: /assets/blog/ctrl/nfq/front.png
front_image_type: contain
front_text: >
    This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based RL algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are neeed to generate control policies of high quality.
---

![Abstract]({{ "/assets/blog/ctrl/nfq/abstract.png" | absolute_url }})

**Title**: Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method<br/>
**Author**: Martin Riedmiller<br/>
**Affiliation**: Professor at [University of Onsabruck](https://www.uni-osnabrueck.de/en/home.html)<br/>
**Current Affiliation**: Research Scientist at [DeepMind](https://deepmind.com)

**Prerequisites**
 - *Reinforcement Learning: An Introduction 2nd Edition* Part 1 (Sutton and Barto, 2018) [[PDF]](https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view)

**Accompanying Resources**
 - *Generalization in reinforcement learning: Safely approximating the value function* (Boyan and Moore, 1995) [[PDF]](https://www.ri.cmu.edu/pub_files/pub1/boyan_justin_1995_1/boyan_justin_1995_1.pdf)
 - *Self-improving reactive agents based on reinforcement learning, planning and teaching* (Lin, 1992) [[PDF]](http://www.incompleteideas.net/lin-92.pdf)
 - *Tree-Based Batch Mode Reinforcement Learning* (Ernst et al., 2005) [[PDF]](http://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf)

<hr/>

## 1  Introduction

To handle real-world problems with massive scale, reinforcement Learning must be accompanied with powerful function approximation methods. However, function approximation is a double-edged sword: its generalization effects can accelerate learning, but weight changes affect unrelated regions, making the agent forget knowledge in regions less frequently visited. Boyan and Moore (1995) function approximators lead to poor convergence properties even in simple problems.

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/BM95.png" alt="BM95"/>
  <figcaption>From <em>Generalization in reinforcement learning: Safely approximating the value function</em> (Boyan and Moore, 1995)</figcaption>
</figure>

To prevent the agent from forgetting meaningful knowledge, we propose reusing previous knowledge. We store all previous experiences in the form of state-action transitions. Then, for every update, the entire batch of experiences is reused to update the neural network. This is a special form of **experience replay** proposed by Lin (1992).

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/Lin92.png" alt="Lin92"/>
  <figcaption>From <em>Self-improving reactive agents based on reinforcement learning, planning and teaching</em> (Lin, 1992)</figcaption>
</figure>

This algorithm belongs to a family of **fitted value iteration** algorithms, a family of value iteration algorithms paired with function approximation. Various function approximations are possible, including randomized trees by Ernst et al. (2005). This algorithm differs by using a multilayered perceptron (MLP), and is therefore called **Neural Fitted Q Iteration** (NFQ). The paper focuses on the following three properties of NFQ:

 1. NFQ is model-free.
 2. NFQ is data-efficient due to experience replay.
 3. NFQ shows comparable results to analytically designed controllers.


## 2  Main Idea

### 2.1  Markovian Decision Processes (MDP)

We expect the readers to be familiar with **Markov Decision Processes** (MDPs). An MDP can be described by a set of states $S$, set of actions $A$, a stochastic transition function $p(s, a, s')$, and a reward or cost function $c: S \times A \to \mathbb{R}$. Unlike most current reinforcement learning papers that use the reward function $r$, this paper uses the cost function $c$. Thus, the objective of the agent is to find an optimal policy $\pi^* : S \to A$ that minimizes the expected cumulative cost for each state.

Vanilla value iteration can only be used in finite number of states and actions. By using function approximation, we can ease the restriction on the number of states and allow $S$ to be continuous.

### 2.2  Classical Q-Learning

We also expect the readers to have basic knowledge of classical tabular **Q-Learning** algorithm. Note that we use the minimum of all actions since we use the cost function $c$.

$$
Q_{k+1} (s, a) := (1 - \alpha) Q(s, a) + \alpha \left( c(s, a) + \gamma \min_b Q_k (s', b) \right)
$$

where $s$ is the state before the transition, $a$ is the action applied, $s'$ is the state after the action, $\alpha$ is the learning rate, and $\gamma$ is the discount factor. Q-learning is guaranteed to converge to the optimal action value $q^*$ for finite $S$ and $A$ if every state-action pair $(s, a) \in S \times A$ is updated infinitely often. In classical Q-Learning, updates are done online in a sample-by-sample manner.

### 2.3  Q-Learning for Neural Networks

To integrate neural networks into Q-learning, one small change must be made. Because Q-values are no longer stored in a tabular manner but instead encoded in the neural network, we need to compose an error function to update the weights of the neural network through backpropagation. A simple error function would be the squared error of the current Q-value and the target Q-value.

$$
error = \left( Q(s, a) - \left( c(s, a) + \gamma \min_b Q(s', b) \right) \right)^2
$$

However, this naive approach of replacing tabular representation with function approximators results in slow learning. With the online update, update from each state action pair affects other states and actions.

## 3  Neural Fitted Q Iteration (NFQ)

### 3.1  Basic Idea

The central idea of NFQ is to replace online single-sample updates with offline batch updates. Using batch updates allows for advanced supervised learning methods that are faster and more robust. We use Rprop by Riedmiller and Braun (1993), which free us from tuning hyperparamters related to the neural network.

A set of experiences $D$ is collected by interacting with real or simulated environment. Each experience is a transition $(s, a, s')$, where $s$ is the original state, $a$ is the chosen action, and $s'$ is the resulting state. It is also possible to include the reward / cost received. However, we perceive the immediate cost as something specified by the designer instead of something observed by the environment. Thus, we leave out costs to specify it at a later point.

### 3.2  The NFQ - Algorithm

The NFQ algorithm consists of two steps:

 1. Generate the training set $P$ from transition samples $D$. $P$ consists of pairs of input $(s, a)$ and target Q-value $c(s, a, s') + \gamma \min_b Q(s', b)$, generated from transitions $(s, a, s') \in D$.
 2. Train the neural network with the training set $P$. This is repeated for several epochs.

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/figure1.png" alt="Main loop of NFQ"/>
  <figcaption></figcaption>
</figure>

### 3.3  Sample setting of Costs

As we have mentioned previously, we do not save the cost in the transition samples $D$. Instead, we design the cost function by partitioning the set of all states $S$ to goal states $S^+$, forbidden states $S^-$, and other states. We define the target differently for each set of states:

<figure>
  <img src="{{absolute_url}}/assets/blog/ctrl/nfq/eqn1.png" alt="Target"/>
  <figcaption></figcaption>
</figure>

If the state ends in goal states $S+$ or forbidden states $S^-$, the episode is terminated. Thus, the target is simply the immediate cost $c(s, a, s')$ or $C^-$. In other states, the costs from the remainder of the episode must also be considered, so $\gamma \min_b Q(s', b)$ must be added.

For simplicity, we use a simplified version where the cost function is constant for all non-forbidden states.

$$
target = 
\begin{cases} 
  c_{trans} & \text{if } s \in S^+ \\
  1 & \text{if } s \in S^- \\
  c_{trans} + \gamma \min Q(s', b) & \text{else (standard case)}
\end{cases}
$$

### 3.4  Variants

## 4  Benchmarking

### 4.1  Type of Tasks
### 4.2  Evaluating Learning Performance
### 4.3  Evaluating Controller Performance

## 5 Empirical Results

### 5.1  The Pole Balancing Task
### 5.2  The Mountain Car Benchmark
### 5.3  The CartPole Regulator Benchmark

## 6 Conclusion

<hr/>

## Final Thoughts


**Recommended Next Papers**
 - Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013) [[Arxiv]](https://arxiv.org/abs/1312.5602)
