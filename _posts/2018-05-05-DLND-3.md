---
playout: post
title: "DLND: 3. Neural Network"
permalink: /mooc/dlnd/3
---

In the [last lesson](/mooc/dlnd/2), we completed Linear Regression, an algorithm of drawing a best-fit linear boundary on the data. However, in many cases, the boundary is far more complex and is separated nonlinearly. In this lesson, we learn how to create a **Neural Network** that can represent and train a nonlinear boundary.

### Nonlinear Data

In the college acceptance model, we previously had a dataset that suggested a linear boundary. However, a different college might have acceptance policy, and it might have some soft minimum test score policy like below.

![Example of Nonlinear boundary]({{ "assets/DLND-3/nonlinear_data.png" | absolute_url }})

We create a model that represents a probability function like below for this nonlinear boundary.

![Probability Function for nonlinear boundary]({{ "assets/DLND-3/nonlinear_model_probability_function.png" | absolute_url }})

### Neural Network

Nonlinear boundaries like these can be simply created by combining multiple linear boundaries like below.

![Combining boundaries]({{ "assets/DLND-3/add_models.png" | absolute_url }})

For each point, each linear model gives some probability $p_1, p_2, \ldots$ that the point is blue. We can add these probabilities $p = p_1 + \ldots$ and use the sigmoid function to clip it between 0 and 1.

![Diagram combining probabilities of linear models]({{ "assets/DLND-3/combine_probabilities.png" | absolute_url }})

We can also add weights and bias to give more authority to one linear model over another.

![Diagram with weights and biases added]({{ "assets/DLND-3/weight_bias_probability.png" | absolute_url }})

This looks just like a perceptron! Thus, we just added another layer of perceptron to our previous layer. This is a **Neural Network**. Neural network is just multiple layers of perceptrons that can be used to model complex nonlinear boundaries.

![Stacking perceptrons to create a neural network]({{ "assets/DLND-3/neural_network.png" | absolute_url }})

### Neural Network Architecture

The neural network is separated into three parts: the **input layer**, the **hidden layers**, and the **output layer**. The input layer is the layer where the input is received. The output layer is the layer with the final probability model. The hidden layers are the layers between the input and output layer.

![Layers of Neural Network]({{ "assets/DLND-3/nn_layers.png" | absolute_url }})

There are a lot of ways to make the neural network model more complex boundaries. First, we can increase the number of nodes in each layer. Adding more nodes in the hidden layer is equivalent to combining more models. Adding more input nodes increases the dimension of the model. Adding more output nodes makes the model solve a multi-class classification problem.

Also, by adding more layers of perceptron, the model is able to draw more and more complex boundaries. These are **Deep Neural Networks**.

![Deep Neural Network]({{ "assets/DLND-3/deep_nn.png" | absolute_url }})

### Training the Neural Network

Now, that we have the architecture of the neural network, we now need to **train** the neural network to model the data well. The procedure is similar to that of Logistic Regression: given an input $x$, we use the model to compute the prediction $\hat{y}$. Then, we compute the error $E$ and use it to update the weights$W$ of the model. We call these steps feedforward and backpropagation.

### Feedforward

**Feedforward** or **Forward Propagation** is the process of using the neural network to predict an output from an input.

![A Neural network with one hidden layer]({{ "assets/DLND-3/feedforward_nn.png" | absolute_url }})

Consider a neural network with one hidden layer. The input is first used in the perceptrons of the hidden layer with the weights $W^{(1)}$ and the sigmoid activatin function $\sigma$. (The superscript in $W$ denotes the layer) Then, the output of the hidden layer is used in the perceptron in the output layer with the weights $W^{(2)}$ and the sigmoid activation function $\sigma$.

The feedforward process can be written in matrix form:

$$ \hat{y} = \sigma (\begin{pmatrix} W_{11}^{(2)} \\ W_{21}^{(2)} \\ W_{31}^{(2)} \end{pmatrix} \sigma (\begin{pmatrix} W_{11}^{(1)} & W_{12}^{(1)} \\ W_{21}^{(1)} & W_{22}^{(1)} \\ W_{31}^{(1)} & W_{32}^{(1)} \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ 1 \end{pmatrix} ))$$

or simply:

$\hat{y} = \sigma ( W^{(2)} \sigma (W^{(1)} x))$

The error function is calculated using the cross entropy again:

$$ E = \sum^m_{i=1} y_i \log \hat{y_i} + (1 - y_i) \log (1 - \hat{y_i})$$

### Backpropagation

To improve the model given the error of the prediction made. we need to use gradient descent again. However, because the model has more layers, it is harder to compute the partial derivatives $\frac{\delta E}{\delta W^{(1)}_{ij}}​$. Therefore, we repeatedly use the chain rule to compute the partial derivatives $\frac{\delta E}{\delta \hat{y}}, \frac{\delta E}{\delta W^{(n)}_{ij}}, \frac{\delta E}{\delta W^{(n-1)}_{ij}}, \ldots​$ to compute $\frac{\delta E}{\delta W^{(1)}_{ij}}​$. This is called **backpropagation**. The name stems from the idea of starting with the (last) output layer and computing partial derivatives backwards to reach the (first) input layer.

After computing the partial derivatives, the weights are updated with the same formula:

$$ W^{(k)}_{ij} := W^{(k)}_{ij} - \alpha \frac{\delta E}{\delta W^{(k)}_{ij}}$$

where $\alpha$ is the learning rate.

