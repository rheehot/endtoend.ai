---
layout: post
title: "DLND: 1. Perceptrons"
permalink: /mooc/dlnd/1
---

**Deep Learning** is used in many places today such as self-driving cars. The core of Deep Learning is a **Neural Network**. For the next three lessons, we will understand how to create a neural network. In this lesson, we will learn about **perceptrons**: the building blocks of neural networks.

### Classification Problems

A **classification problem** is a problem where given a data, we want to classify the data into one of few categories. When there are only two categories, it is called a **binary classification problem**.

One example of a classification problem is predicting if a student will get accepted to a university based on his or her test score and grade.

![An example of a classification problem]({{ "assets/DLND-1/classification_problem.png" | absolute_url }})

In this data, a simple diagonal line seems to separate the data pretty well.

![A linear boundary for the classification problem]({{ "assets/DLND-1/classification_line.png" | absolute_url }})

In reality, universities look at more other than grades and tests scores. Let's add another element (class rank) to the graph. Then, a plane would separate this data pretty well.

![A 3D classification problem]({{ "assets/DLND-1/3d_classification.png" | absolute_url }})

The plane has the equation $w_1x_1 + w_2x_2 + w_3x_3 + b = 0$, abbreviated as $Wx + b = 0$. For a student with class rank $x_1$, test score $x_2$, and grades $x_3$, if $Wx + b \geq 0$, we predict **1**, as in the student will be accepted, and if $Wx + b < 0$, we predict **0**, as in the student will be rejected. We denote our **prediction** as $\hat{y}$.

To be more realistic, we need to add even more features to consider. It is now difficult to plot a graph, but the basic idea is the same. For $n$ features, we need a $n$-dimensional space. The boundary would be a $n-1$-dimensional hyperplane $Wx + b = 0$ that separates the students. The prediction $\hat{y}$ is still calculated the same way.

### Perceptrons

**Perceptrons** are building blocks of a Neural Network that encapsulates the idea above. Based on input data, the perceptron outputs the classification.

![A Diagram of a perceptron]({{ "assets/DLND-1/perceptron.png" | absolute_url }})

Like above, perceptrons can generalize to $n$ features.

![Diagram of generalized perceptron]({{ "assets/DLND-1/perceptron_general.png" | absolute_url }})

Note that the perceptron actually does two things: it calculates $Wx + b$ given $x = (x_1, x_2, \ldots, x_n)$, and it compares $Wx + b$ with $0$. The two steps are respectively called the **linear function** and the **activation function**. This classification is useful since there are many possible activation functions.

![Linear Function and Activation Function]({{ "assets/DLND-1/perceptron_two_functions.png" | absolute_url }})

Also note that bias can be thought of as a separate input with weight $b$, or inherent parameter of the perceptron. In this course, we give separate input for the bias.

![Different ways of representing bias in perceptrons]({{ "assets/DLND-1/perceptron_bias.png" | absolute_url }})

### Perceptrons as Logical Operators

Perceptrons can be used as logical operators such as **AND**, **OR**, **NOT**.

![The AND perceptron]({{ "assets/DLND-1/perceptron_and.png" | absolute_url }})

![The OR perceptron]({{ "assets/DLND-1/perceptron_or.png" | absolute_url }})

For more complex operators like **XOR**, we need to connect multiple perceptrons.

![Multi-layered perceptrons for XOR]({{ "assets/DLND-1/perceptron_xor.png" | absolute_url }})

### Perceptron Algorithm

For basic logical operators, we can build them ourselves. However, in reality, we want to automate the process. Given some labeled points, we want to fit a line so that the perceptron correctly classifies most (if not all) points.

In the example below, a point is misclassified. For the point to be classified correctly, the line must move towards the point.

![A diagram with a misclassified point]({{ "assets/DLND-1/misclassified_point.png" | absolute_url }})

One way to move the line closer is by subtracting the point from the line weights. (Or if the positive point is in negative area, add) However, since this might move the line too drastically, we use the **learning rate** to move the line marginally.

![Modifying the boundary to fix the misclassification]({{ "assets/DLND-1/misclassified_point_math.png" | absolute_url }})

The **Perceptron algorithm** repeatedly performs this process of modifying the line with all misclassified points.

