---
layout: post
title: "GSoC TensorFlow Part 6: Evaluating RND on Mountain Car"
author: Seungjae Ryan Lee
permalink: /blog/gsoc/6
redirect_from:
 - /gsoc/6
tags:
 - reinforcement-learning
 - gsoc
 - tensorflow

image: /assets/blog/gsoc/gsoc_logo.png
image_type: contain
excerpt: "After finishing implementing Random Network Distillation by Burda et al., now it is time to evaluate the algorithm in various hard exploration environments. I first start the evaluation on Mountain Car, a simple environment that requires extensive exploration to reach the goal state."

nav:
- name: "Mountain Car"
  permalink: "#mountain-car"
- name: "Evaluation of RND"
  permalink: "#evaluation-of-rnd"
- name: "What's Next?"
  permalink: "#next"

related:
- title: "GSoC TensorFlow Part 5: Implementing the Core of RND"
  link: /gsoc/5
  image: /assets/blog/gsoc/gsoc_logo.png
  image_type: contain
- title: "GSoC TensorFlow Part 7: Retrospective"
  link: /gsoc/7
  image: /assets/blog/gsoc/gsoc_logo.png
  image_type: contain
---

Time flies! It's already mid-August, and I received an email about project submission guidelines for the final evaluation. Luckily, with the help of Oscar and Mandar, I was able to resolve many bugs in my implementation of Random Network Distillation! I have evaluated and verified that Random Network Distillation encourages exploration of the PPO agent, so in this post I introduce the Mountain Car environment and share the performance of RND.

## Mountain Car

To evaluate the effectiveness of RND in encouraging exploration, Random Network Distillation was run on `MountainCar-v0`.

<div class="w60">
  <video autoplay muted loop controls style="width: 100%;">
    <source src="{{ absolute_url }}/assets/blog/gsoc/6/mountaincar.mp4" type="video/mp4">
  </video>
</div>

MountainCar is a reinforcement learning environment where the agent's goal is to move a car up a steep valley and reach the flag. Because the valley is very steep, the car cannot simply climb up the hill with its own strength. Instead, the car must move back and forth to gain momentum.

Mountain Car is a simple environment with $R^2$ observation space and three actions possible.  Yet, famous baseline algorithms such as Deep Q-Network (DQN) or Proximal Policy Optimization (PPO) have trouble finding optimal solution due to insufficient exploration. Thus, it is a suitable environment to test the RND agent.

For every timestep, a reward of -1 is given. To ensure that each episode does not take too long, a time limit of 200 is enforced. In other words, the worst episodic return is -200.

## Evaluation of RND

RND was tested on Mountain Car with 10000 global steps, where the agent was evaluated with its current policy every 500 steps. The hyperparameters were not tuned: I used the default hyperparameters in TF-Agents.

<div class="w80">
  <img class="mdl-cell mdl-cell--6-col mdl-cell--6-col-desktop mdl-cell--4-col-tablet mdl-cell--12-col-phone" src="{{ absolute_url }}/assets/blog/gsoc/6/rnd_vs_ppo_eval.png" alt="">
  <figcaption style="text-align: center;">Average return during evaluation. Orange is RND, Red is PPO.</figcaption>
</div>

During each evaluation, the average return of trajectories were calculated. The orange line shows RND and the red line shows PPO. PPO agent always terminates the episode through time limit: it never reaches the flag at the top of the mountain. In contrast, the RND agent is able to reach the goal in the first evaluation.


<div class="w80 mdl-grid">
  <img class="mdl-cell mdl-cell--6-col mdl-cell--6-col-desktop mdl-cell--4-col-tablet mdl-cell--12-col-phone" src="{{ absolute_url }}/assets/blog/gsoc/6/rnd_vs_ppo_train.png" alt="">
  <img class="mdl-cell mdl-cell--6-col mdl-cell--6-col-desktop mdl-cell--4-col-tablet mdl-cell--12-col-phone" src="{{ absolute_url }}/assets/blog/gsoc/6/avg_rnd_loss.png" alt="">
  <div style="clear: both;"></div>
  <figcaption style="text-align: center;">Average return and RND loss during training. Dark blue is RND. Light blue is PPO.</figcaption>
</div>

Inspired by the original RND paper (Burda et al., 2018), I also plotted the average RND loss (intrisic reward) with average return during training. The peak of average RND loss approximately matches the timestep that the average return starts to rise. The agent receives unfamiliar observations as it goes closer to the flag, which results in a high prediction loss. Since the RND loss is also the intrinsic reward, the agent is rewarded to continue visiting this state.

## What's Next? {#next}

With Mountain Car, I verified the performance and utility of RND. Now, all that is left is to evaluate RND on Atari environments known for hard exploration and sparse rewards!

<figure class="w80">
  <img src="{{ absolute_url }}/assets/blog/gsoc/6/atari_difficulty.png" alt="">
  <figcaption style="text-align: center;">From <i>Unifying Count-Based Exploration and Intrinsic Motivation</i> (Bellemare et al., 2016)</figcaption>
</figure>

Out of 57 Atari environments, Burda et al. tested RND on the 6 of the 7 hard-exploration sparse-reward environments commonly called "Atari Hard." (excluding Freeway)

<figure class="w80">
  <img src="{{ absolute_url }}/assets/blog/gsoc/6/rnd_paper_plots.png" alt="">
  <figcaption style="text-align: center;">From <i>Exploration by Random Network Distillation</i> (Burda et al., 2018)</figcaption>
</figure>

The difference between PPO and RND is most apparent in **Montezuma's Revenge** and **Venture**, so these will be my next evaluation environments!
