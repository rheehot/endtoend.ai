---
published: false
layout: post
title: "GSoC TensorFlow Part 3: Going Down the Rabbit Hole"
author: Seungjae Ryan Lee
permalink: /gsoc/3
tags:
 - reinforcement-learning
 - gsoc
 - tensorflow

image: /assets/blog/gsoc/gsoc_logo.png
image_type: contain
excerpt: "This week, I explored the environment wrappers of TF-Agents. These environment wrappers allow TF-Agents to be used in a multitude of environments, including those in OpenAI Gym."

related:
- title: "GSoC TensorFlow Part 2: Improving Documentation"
  link: /gsoc/2
  image: /assets/blog/gsoc/gsoc_logo.png
  image_type: contain
---

**Discrete action spaces** are action spaces where only a few distinct action is possible. For example, in CartPole, the pole balancing environment, the action space is defined as \{ LEFT, RIGHT \}. **Multi-discrete action spaces** are action spaces that consists of several discrete action subspaces. For example, the [Obstacle Tower environment](https://www.endtoend.ai/obstacle-tower/3#action-space) provided by Unity has four subspaces: \{ NOOP, FORWARD, BACKWARD \}, \{ NOOP, LEFT, RIGHT \}, \{ NOOP, TURN LEFT, TURN RIGHT \}, \{ NOOP, JUMP \}.

[TF-Agents Issue #97](https://github.com/tensorflow/agents/issues/97) is about adding support for these "multi-discrete" action spaces.

### action_spec

The issue points at the [tf_agents/agents/dqn/dqn_agent.py](https://github.com/tensorflow/agents/blob/master/tf_agents/agents/dqn/dqn_agent.py#L172):

```python
flat_action_spec = tf.nest.flatten(action_spec)
self._num_actions = [
    spec.maximum - spec.minimum + 1 for spec in flat_action_spec
]

# TODO(oars): Get DQN working with more than one dim in the actions.
if len(flat_action_spec) > 1 or flat_action_spec[0].shape.ndims > 1:
  raise ValueError('Only one dimensional actions are supported now.')
```

In this code snippet, it seems like `flat_action_spec` is a flattened version of `action_spec`, which stores information about the environment's action space. Let's see where they are from:

```python
@gin.configurable
class DqnAgent(tf_agent.TFAgent):
  """A DQN Agent.
  ...
  """

  def __init__(
      self,
      time_step_spec,
      action_spec,
      ...
      name=None):
    """Creates a DQN Agent.

    Args:
      time_step_spec: A `TimeStep` spec of the expected time_steps.
      action_spec: A nest of BoundedTensorSpec representing the actions.
      ...
    """
```

It is given to the `DqnAgent` as a parameter. The docstring says mentions `BoundedTensorSpec` , which is an unfamiliar term.

### BoundedTensorSpec

Searching `BoundedTensorSpec` in the TF-Agents codebase yielded a long list of results, so it seems like it is a widely-used type in this repository. The definition could be found in [tf_agents/specs.py](https://github.com/tensorflow/agents/blob/master/tf_agents/agents/specs.py#L33):

```python
from tensorflow.python.framework import tensor_spec as ts  # TF internal

tfd = tfp.distributions

TensorSpec = ts.TensorSpec
BoundedTensorSpec = ts.BoundedTensorSpec
```

Apparently it is from defined inside TensorFlow! There is a good documentation in the [official website](https://www.tensorflow.org/api_docs/python/tf/contrib/framework/BoundedTensorSpec), and also in the [TensorFlow codebase](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/framework/tensor_spec.py#L120). `BoundedTeosrSpec`  is a `TensorSpec` that specifies minimum and maximum values, and a `TensorSpec` is metadata for describing the `tf.Tensor` objects accepted or returned by some TensorFlow APIs.

Now let's go back to `DqnAgent`. `action_spec` is passed as a parameter, so let's find where the agent is initialized.

### tf_env.action_spec()

This is where the example scripts provided by TF-Agents are useful: let's take a look at [tf_agents/agents/dqn/examples/v1/train_eval_gym.py](https://github.com/tensorflow/agents/blob/master/tf_agents/agents/dqn/examples/v1/train_eval_gym.py).

```python
agent_class = dqn_agent.DdqnAgent if FLAGS.use_ddqn else dqn_agent.DqnAgent
train_eval(
    FLAGS.root_dir,
    agent_class=agent_class,
    num_iterations=FLAGS.num_iterations)
```

First, they specify whether to use Double DQN (DDQN) or DQN . Then, they use this `agent_class`  to define `tf_agents`:

```python
tf_agent = agent_class(
        tf_env.time_step_spec(),
        tf_env.action_spec(),
        q_network=q_net,
        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),
        epsilon_greedy=epsilon_greedy,
        target_update_tau=target_update_tau,
        target_update_period=target_update_period,
        td_errors_loss_fn=dqn_agent.element_wise_squared_loss,
        gamma=gamma,
        reward_scale_factor=reward_scale_factor,
        gradient_clipping=gradient_clipping,
        debug_summaries=debug_summaries,
        summarize_grads_and_vars=summarize_grads_and_vars,
        train_step_counter=global_step)
```

So `tf_env` has a `action_space()` method that returns a `BoundedTensorSpec` type. `tf_env` is defined a few lines above:

```python
tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))
eval_py_env = suite_gym.load(env_name)
```

So `tf_env` is an instance of the `TFPyEnvironment` class that requires some parameter. Let's take a look at what `suite_gym.load(env_name)` does first, and then peek at the definition of `tf_py_environment.TFPyEnvironment`.

### suite_gym.load(env_name)

[tf_agents/environments/suite_gym.py](https://github.com/tensorflow/agents/blob/master/tf_agents/environments/suite_gym.py) has a `load()` method that "loads the selected [OpenAI Gym] environment and wraps it with the specified wrappers."

```python
@gin.configurable
def load(environment_name,
         discount=1.0,
         max_episode_steps=None,
         gym_env_wrappers=(),
         env_wrappers=(),
         spec_dtype_map=None):
  """Loads the selected environment and wraps it with the specified wrappers.
  Note that by default a TimeLimit wrapper is used to limit episode lengths
  to the default benchmarks defined by the registered environments.
  Args:
    environment_name: Name for the environment to load.
    discount: Discount to use for the environment.
    max_episode_steps: If None the max_episode_steps will be set to the default
      step limit defined in the environment's spec. No limit is applied if set
      to 0 or if there is no timestep_limit set in the environment's spec.
    gym_env_wrappers: Iterable with references to wrapper classes to use
      directly on the gym environment.
    env_wrappers: Iterable with references to wrapper classes to use on the
      gym_wrapped environment.
    spec_dtype_map: A dict that maps gym specs to tf dtypes to use as the
      default dtype for the tensors. An easy way how to configure a custom
      mapping through Gin is to define a gin-configurable function that returns
      desired mapping and call it in your Gin congif file, for example:
      `suite_gym.load.spec_dtype_map = @get_custom_mapping()`.
  Returns:
    A PyEnvironment instance.
  """
  gym_spec = gym.spec(environment_name)
  gym_env = gym_spec.make()

  if max_episode_steps is None and gym_spec.timestep_limit is not None:
    max_episode_steps = gym_spec.max_episode_steps

  return wrap_env(
      gym_env,
      discount=discount,
      max_episode_steps=max_episode_steps,
      gym_env_wrappers=gym_env_wrappers,
      env_wrappers=env_wrappers,
      spec_dtype_map=spec_dtype_map)
```

It creates an OpenAI Gym environment, and updates the time limit for the environment if needed. It uses `wrap_env()`, which is also defined in the same file:

```python
@gin.configurable
def wrap_env(gym_env,
             discount=1.0,
             max_episode_steps=None,
             gym_env_wrappers=(),
             time_limit_wrapper=wrappers.TimeLimit,
             env_wrappers=(),
             spec_dtype_map=None,
             auto_reset=True):
  """Wraps given gym environment with TF Agent's GymWrapper.
  Note that by default a TimeLimit wrapper is used to limit episode lengths
  to the default benchmarks defined by the registered environments.
  Args:
    gym_env: An instance of OpenAI gym environment.
    discount: Discount to use for the environment.
    max_episode_steps: If None the max_episode_steps will be set to the default
      step limit defined in the environment's spec. No limit is applied if set
      to 0 or if there is no timestep_limit set in the environment's spec.
    gym_env_wrappers: Iterable with references to wrapper classes to use
      directly on the gym environment.
    time_limit_wrapper: Wrapper that accepts (env, max_episode_steps) params to
      enforce a TimeLimit. Usuaully this should be left as the default,
      wrappers.TimeLimit.
    env_wrappers: Iterable with references to wrapper classes to use on the
      gym_wrapped environment.
    spec_dtype_map: A dict that maps gym specs to tf dtypes to use as the
      default dtype for the tensors. An easy way how to configure a custom
      mapping through Gin is to define a gin-configurable function that returns
      desired mapping and call it in your Gin config file, for example:
      `suite_gym.load.spec_dtype_map = @get_custom_mapping()`.
    auto_reset: If True (default), reset the environment automatically after a
      terminal state is reached.
  Returns:
    A PyEnvironment instance.
  """

  for wrapper in gym_env_wrappers:
    gym_env = wrapper(gym_env)
  env = gym_wrapper.GymWrapper(
      gym_env,
      discount=discount,
      spec_dtype_map=spec_dtype_map,
      auto_reset=auto_reset,
  )

  if max_episode_steps > 0:
    env = time_limit_wrapper(env, max_episode_steps)

  for wrapper in env_wrappers:
    env = wrapper(env)
```

 `gym_env_wrappers` and `env_wrappers` are both empty in our case, so the only line that is actually executed is  `env = gym_wrapper.GymWrapper(gym_env, ...)` .

### GymWrapper

GymWrapper is defined in [tf_agents/environments/gym_wrappers.py](https://github.com/tensorflow/agents/blob/master/tf_agents/environments/gym_wrapper.py#L98):

```python
def __init__(self,
             gym_env,
             discount=1.0,
             spec_dtype_map=None,
             match_obs_space_dtype=True,
             auto_reset=True):
  super(GymWrapper, self).__init__(gym_env)

  self._gym_env = gym_env
  self._discount = discount
  self._action_is_discrete = isinstance(self._gym_env.action_space,
                                        gym.spaces.Discrete)
  self._match_obs_space_dtype = match_obs_space_dtype
  # TODO(sfishman): Add test for auto_reset param.
  self._auto_reset = auto_reset
  self._observation_spec = _spec_from_gym_space(
      self._gym_env.observation_space, spec_dtype_map)
  self._action_spec = _spec_from_gym_space(self._gym_env.action_space,
                                           spec_dtype_map)
  self._flat_obs_spec = tf.nest.flatten(self._observation_spec)
  self._info = None
  self._done = True
```



```python
def _spec_from_gym_space(space, dtype_map=None):
  """Converts gym spaces into array specs.
  Gym does not properly define dtypes for spaces. By default all spaces set
  their type to float64 even though observations do not always return this type.
  See:
  https://github.com/openai/gym/issues/527
  To handle this we allow a dtype_map for setting default types for mapping
  spaces to specs.
  TODO(oars): Support using different dtypes for different parts of the
  observations. Not sure that we have a need for this yet.
  Args:
    space: gym.Space to turn into a spec.
    dtype_map: A dict from specs to dtypes to use as the default dtype.
  Returns:
    A BoundedArraySpec nest mirroring the given space structure.
  Raises:
    ValueError: If there is an unknown space type.
  """
  if dtype_map is None:
    dtype_map = {}

  if isinstance(space, gym.spaces.Discrete):
    # Discrete spaces span the set {0, 1, ... , n-1} while Bounded Array specs
    # are inclusive on their bounds.
    maximum = space.n - 1
    # TODO(oars): change to use dtype in space once Gym is updated.
    dtype = dtype_map.get(gym.spaces.Discrete, np.int64)
    return specs.BoundedArraySpec(
        shape=(), dtype=dtype, minimum=0, maximum=maximum)
  elif isinstance(space, gym.spaces.MultiDiscrete):
    dtype = dtype_map.get(gym.spaces.MultiDiscrete, np.int32)
    minimum = np.zeros_like(space.nvec, dtype=dtype)
    maximum = np.asarray(space.nvec - 1, dtype=dtype)
    return specs.BoundedArraySpec(
        shape=space.shape, dtype=dtype, minimum=minimum, maximum=maximum)
  elif isinstance(space, gym.spaces.MultiBinary):
    dtype = dtype_map.get(gym.spaces.MultiBinary, np.int8)
    shape = (space.n,)
    minimum = np.zeros(shape, dtype=dtype)
    maximum = np.ones(shape, dtype=dtype)
    return specs.BoundedArraySpec(
        shape=shape, dtype=dtype, minimum=minimum, maximum=maximum)
  elif isinstance(space, gym.spaces.Box):
    # TODO(oars): change to use dtype in space once Gym is updated.
    dtype = dtype_map.get(gym.spaces.Box, np.float32)
    minimum = np.asarray(space.low, dtype=dtype)
    maximum = np.asarray(space.high, dtype=dtype)
    return specs.BoundedArraySpec(
        shape=space.shape, dtype=dtype, minimum=minimum, maximum=maximum)
  elif isinstance(space, gym.spaces.Tuple):
    return tuple([_spec_from_gym_space(s, dtype_map) for s in space.spaces])
  elif isinstance(space, gym.spaces.Dict):
    return collections.OrderedDict([(key, _spec_from_gym_space(s, dtype_map))
                                    for key, s in space.spaces.items()])
  else:
    raise ValueError(
        'The gym space {} is currently not supported.'.format(space))
```







### TFPyEnvironment

Looks like we have another custom defined class. Let's look at `TFPyEnvironment`

