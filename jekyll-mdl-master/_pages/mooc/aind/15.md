---
layout: page
title: "AIND: 15. Bayes' Theorem and Naive Bayes"
permalink: /mooc/aind/15
---

In the [last lesson](/mooc/aind/14), we learned about basic probability and used it to deduce a surprising result in the cancer example. In this lesson, we will introduce Bayes Rule and Naive Bayes classifiers.

### Bayes' Theorem

In the cancer example, we calculated the $P(C \mid +)$, probability of the patient having cancer given that the patient got a positive test result. To calculate this, we used the product rule: $P(C, +) = P(C \mid +) P(+) = P(+ \mid C) P(C)$. By reordering the terms, we discover the following equation named the **Bayes' Theorem** or **Bayes' Rule**:

$$ P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$

The Bayes' Theorem uses the probability $P(B \mid A)$ to compute $P(A \mid B)$. This is  useful in many cases: when $A$ is a symptom, and $B$ is the disease, or when $A$ is the cause, and $B$ is the effect.

Often, it is impossible to calculate $P(B)$ directly. Thus, $P(B)$ is calculated with total probability, leading to the alternative form of Bayes' Theorem:

$$ P(A \mid B) = \frac{P(B \mid A) P(A)}{\sum_a P(B \mid A = a) P(A = a)} $$

It is also possible to avoid explicitly calculating $P(B)$. $P(B)$ is constant for all event $A$, so we can simply find the relative ratio among $P(a_1 \mid B), P(a_2 \mid B), \ldots, P(a_n \mid B)$ and normalize them. In other words,

$$P(A \mid B) \propto P(B \mid A) P(A)$$

### Naive Bayes

It is normal to have multiple evidence for a single event. Suppose that we have a disease $D$ and multiple symptoms $S_1, \ldots, S_n$.  We want to find the probability $P(D \mid S_1, \ldots, S_n)$. Assume each symptom is binary: a patient can either have a symptom or not. Then, there are $2^n$ possible combinations of symptoms. For each combination $s_1, \ldots s_n$, we need to go through the dataset and check how many have the disease $D$.  This is unfeasible for large $n$.

We can reformulate this with Bayes' Theorem:

$$ P(D \mid S_1, \ldots, S_n) \propto P(S_1, \ldots, S_n \mid D) P(D)$$

But still, there are $2^n$ combinations, so we need to go through the dataset $2^n$ times. To solve this problem of $O(2^n)$, we use a naive assumption that $S_1, \ldots, S_n$ are conditionally independent given $D$. Then, we can rewrite the equation:

$$ P(D \mid S_1, \ldots, S_n) \propto P(S_1 \mid D) \ldots P(S_n \mid D) P(D) $$

Now, we don't need to compute probabilities for $2^n$ combinations: we can think of each symptom separately and compute $n$ probabilities. While $O(2^n)$ was not feasible for large $n$, $O(n)$ is.

We can rewrite our assumption with a full joint distribution:

$$ P(D, S_1, \ldots, S_n) = P(D) \prod_i P(S_i \mid D)$$

This model is called the **Naive Bayes** model.

### Spam Email Classifier

Suppose that we want to classify an email as *spam* or *ham* (not spam). We have a dataset of correctly classified emails, and with it we want to calculate the probability that a new email is spam. (We can use more complex features like the order of the words, but even without it we can create a decent spam email classifier.)

![Data of Spam and Ham emails]({{ "assets/mooc/aind/15/spam_ham.png" | absolute_url }})

Suppose we want to classify the email "Easy money." In other words, we want to calculate $P(\text{spam} \mid \text{"easy"}, \text{"money"})$. Let us use Bayes' Theorem:

$$ P(\text{spam} \mid \text{"easy"}, \text{"money"}) \propto P(\text{"easy"}, \text{"money"} \mid \text{spam}) P(\text{spam}) $$

From the dataset, we know that $P(\text{spam}) = 3/8$. Let's compute $P(\text{"easy"}, \text{"money"} \mid \text{spam})$. To compute this, we assume that "money" and "spam" are conditionally independent given that the email is spam. In other words:

$$P(\text{"easy"}, \text{"money"} \mid \text{spam}) = P(\text{"easy"} \mid \text{spam}) P(\text{"money"} \mid \text{spam}) = 2 / 9$$

Therefore,

$$ P(\text{spam} \mid \text{"easy"}, \text{"money"}) \propto 2/9 \times 3/8 = 1/12$$

Similarly, we can calculate the ratio of the probability that an email is ham given that it contains the word "easy" and "money":

$$P(\text{ham} \mid \text{"easy"}, \text{"money"}) \propto 1/40$$ 

Now, we can normalize the ratios to get the actual probabilities.

$$P(\text{spam} \mid \text{"easy"}, \text{"money"}) = \frac{1/12}{1/12 + 1/40} = 10/13 \simeq 77\%$$

