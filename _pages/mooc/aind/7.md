---
layout: page
title: "AIND: 7. Introduction to Planning"
permalink: /mooc/aind/7/
---

### Limitations of Problem Solving Agent

In Problem Solving, the agent plans the optimal path before taking actions and executes the optimal path without any feedback. In finding the path from Arad to Bucharest, the problem solving agent successfully found the optimal path through A* search.

![Example of Problem Solving Agent]({{ "assets/mooc/aind/7/arad_to_bucharest.png" | absolute_url }})

However, there are certain environments where planning without execution leads to suboptimal results.

* **Stochastic** environment
  * If we don't know the outcome of an action, we need to be able to deal with contingencies.
* **Multi-agent** environment
  * If there are other agents, we need to be able to react according to their actions.
* **Partially Observable** environment
  * If we only know parts of the environment, we do not have enough information to plan the entire path. For example, the road from Sibiu to Rimnicu Vilcea might be closed, but you do not know whether it is closed until you are in Sibiu.

Most of these problems can be solved by changing our point of view and planning with "belief states".

### Non Observable Vacuum World

![Diagram of Vacuum World]({{ "assets/mooc/aind/7/vacuum_world.png" | absolute_url }})

Let's go back to Vacuum World. There is a room with two locations A (left) and B (right), and dirt can be in either locations. There is a vacuum cleaner in one location. There are 3 possible actions: move left, move right, and suck dirt.

Suppose that this time, the vacuum cleaner's sensors broke down, so it is unable to detect its location or the existence of dirt in its location. Therefore, in the beginning, we can be in any of the 8 states.

![Belief State Diagram]({{ "assets/mooc/aind/7/belief_state_diagram.png" | absolute_url }})

However, by taking an action, we can learn more about the world without observation. By taking the 'move right' action, we can now eliminate all states with the robot on the left. These sets of states in the diagram above are called the **belief states**, as they represent the belief of the agent.

### Partially Observable Vacuum World

Now, suppose that the vacuum cleaner knows its location and whether dirt exists in its location, but does not know about dirt on the other location. Here is a partial diagram of the partially observable Vacuum World.

![Diagram of Partial Observable Vacuum World]({{ "assets/mooc/aind/7/partially_observable_diagram.png" | absolute_url }})

In a partially observable world, we cycle through actions and observations. Suppose that we believe that we are in either state 1 or 3. We take an action "Right", and afterwards we observe the state. Taking an action either reduces or preserves the size of the belief state, because we are in a deterministic world. Observing the environment partitions the belief state.

### Stochastic Vacuum World

Now, let's consider a world where the robot has slippery wheels and might fail to move. (The suck action still works perfectly.) Here is a partial diagram for the stochastic Vacuum World:

![Diagram for stochastic Vacuum World]({{ "assets/mooc/aind/7/stochastic_diagram.png" | absolute_url }})

In this case, when the cleaner is in the left, the right action might or might not move the robot, so there are two possible states after the action. Therefore, in a stochastic environment an action can have multiple outcomes, so taking an action can increase the number of possible states. Observation still partitions the belief state.

In this world, there exists no finite plan that will guarantee that both locations are clean. Therefore, we need to introduce an infinite plan, and to represent it, we use a tree structure below instead of a sequence structure [S, R, S].

![A notation for Infinite Plan]({{ "assets/mooc/aind/7/infinite_plan.png" | absolute_url }})

The above diagram represents an infinite plan. We start with a suck action. Then, we try to move right until we observe that we have moved to location B. Then, we perform another suck action. With this plan, we can guarantee that this plan will reach the goal *eventually*.

### Finding a Successful Plan

Like Problem Solving, we can find a successful plan through search. The search tree has the individual states as nodes, and it has both actions and loops as edges.

![A tree to find a successful plan]({{ "assets/mooc/aind/7/search_plan_tree.png" | absolute_url }})

From the big search tree, we can choose some branch as a single plan. For this plan to be an unbounded solution, every leaf node of the branch must be a goal, and for it to be bounded, the branch must not contain any loops.

### Problem Solving via Mathematical Notation

It is possible to interpret a plan with First Order Logic. Suppose we have a plan [A, S, F] for some deterministic environment. One step of the plan can be written with a relation:

$$s' = \text{Result}(s, a)$$

where $\text{Result}(s, a)$ returns a resulting state of starting in state $s$ and applying action $a$. The entire plan can be verified with the following statement:

$$\text{IsGoal}(\text{Result}(\text{Result}(A, A\to S), S \to F))$$

where $\text{IsGoal}(s)$ checks if a state $s$ is a goal state.

![Problem Solving Diagram for Stochastic Environment]({{ "assets/mooc/aind/7/stochastic_problem_solving_diagram.png" | absolute_url }})

The stochastic environment needs more complex statements, since it has belief states instead of individual states. Consider a fully observable Vacuum World where dirt could randomly appear at each location. Then, one action-observation step can be represented with the following statement:

$$b' = \text{Update}(\text{Predict}(b, a), o)$$

where $\text{Predict}(b, a)$ returns a larger belief state from starting in belief state $b$ taking action $a$, and $\text{Update}(b, o)$ returns the belief state after updating belief state $b$ with observation $o$.
