---
layout: page
title: "AIND: 12. Improving Minimax Search"
permalink: /mooc/aind/12/
---

In the [last lesson](/mooc/aind/11), we learned about the **Minimax Search**. In this lesson, we will learn how to improve the minimax search through Iterative Deepening, Quiescent Search, Alpha-Beta Pruning, and Opening Book.

### Iterative Deepening

One improvement for depth-limited search is **Iterative Deepening**. Instead of setting a search depth $d$ that the computer can search in time, we search with depth $1$, then with depth $2$, and continue incrementing the depth until we run out of time.

Although some time is wasted, the amount of time wasted is small compared to the amount of time spent searching the last level due to the exponential nature of the problem. For branching factor of $b$, only about $1 / b$ of the nodes searched in iterative deepening is repetitive.

![Iterative Deepening in Binary Tree]({{ "assets/mooc/aind/12/binary_iterative_deepening.png" | absolute_url }})

### Horizon Effect

The problem with depth-limited search is that we might not be searching deep enough. For example, in chess, there might be a branch in the game tree with a queen capture on depth $d$, but that capture leads to the opponent's checkmate in depth $d+1$. If the depth-limited search only went until depth $d$, it would believe that this branch is a great branch and pursue it. This is called the **horizon effect**. The computer can only see up to depth $d$ (the "horizon"), causing it to favor suboptimal behavior.

### Quiescent Search

One way to solve the horizon effect is to check for critical decisions. If there are signs of critical decisions being made in the leaf nodes, we might want to look deeper in the game tree. This is called the **Quiescent search**. We distinguish "quiet" and "noisy" moves by detecting wild fluctuation in evaluation values over several ply. Then, we look deeper on the "noisy" moves to get a better estimate of the result.

### Other Evaluation Functions

Another way to combat horizon effect is to use a better evaluation function. Accurate evaluation functions will evaluate the board position better, so there is less possibility that the evaluation function changes dramatically after few moves.

However, complex evaluation functions take longer to compute. Because we need to evaluate every node, a complex evaluation function could decrease the possible search depth considerably. In the end, the dominant strategy can only be determined through testing: deep searches with simple evaluation function, or shallower searches with complex evaluation function.

Here is one different evaluation function: the difference of moves available for the computer and the moves available for the opponent. It is almost as simple as our previous evaluation function but uses more information of the board.

$$ \text{# of my moves - # of opponent's moves}$$

To promote aggressive or defensive behavior, we can multiply constants to the terms.

$$ \text{# of my moves - 2} \times \text{# of opponent's moves}$$

$$ 2 \times \text{# of my moves - # of opponent's moves}$$

### Alpha-Beta Pruning

One significant way to improve the minimax algorithms is to **prune** the game tree. Pruning is a technique of removing sections of the tree that are unlikely or impossible to provide useful information. Specifically, we will use **Alpha-Beta pruning**, the pruning technique that prunes the game tree considerably while generating same answer as the basic minimax algorithm.

The intuition behind alpha-beta pruning is simple. If we know that branch $A$ has an evaluation value of 2, and branch $B$ has a value of at most 1, we don't need to search through $B$, since we will choose branch $A$. Similarly, if the opponent has a choice between branch $A'$ with a value of at least 0 and $B'$ with a value of -1, then the opponent will choose branch $B'$.

![Example of alpha-beta pruning]({{ "assets/mooc/aind/12/pruning_ex.jpg" | absolute_url }})

For example, consider a simple tree above. To evaluate the value of the head node, we need to compute the following statement:

$$\max(\min(3, 5, 10), \min(2, a, b), \min(2, 7, 3)) $$

Despite not knowing the values $a,b$, we can calculate this line. We know that $\min(2, a, b) \leq 2 < 3$, so $\max(3, \min(2, a, b), 2)$ is $3$. Similarly, we did not need the value  $7$ and $3$ in the last branch.

While naive minimax searches through the game tree with order $b^d$, alpha-beta pruning can search through the game tree with $b^{3d / 4}$ for random game tree, and $b^{d / 2}$ if the game tree is optimized to find best moves first.

### Implementing Alpha-Beta Pruning

To use alpha-beta pruning, for each node we need to track two variables: **alpha** and **beta**. $\alpha$ represents the value of the best explored option for the maximizer along the path to the root. $\beta$ represents the best explored option for the minimizer along the path to the root. We start with alpha as $-\infty$ and beta as $\infty$, since we explored nothing.

We update alpha and beta of a node as we search through its descendants. In every node, alpha and beta are initialized with the values of its parent. In a maximizing node, alpha is updated if the new value from its children is higher than alpha. In a minimizing node, beta is updated if the new value from its children is higher than beta. In other words, for new value $v$:

$$ \alpha := \max(v, \alpha) $$

in a maximizing node, and 

$$ \beta := \min(v, \beta) $$

in a minimizing node.

![Example of Alpha Beta cutoffs]({{ "assets/mooc/aind/12/alphabeta_ex_2.png" | absolute_url }})

Consider the left tree in the example above. We start with $(\alpha_A, \beta_A) = (-\infty, \infty)$. The maximizing node $A$ has children $B$ with score 4. Therefore, $\alpha_A = \max(4, -\infty)$, signifying that $A$ has value of at least $4$. We pass the parameters $(\alpha_A, \beta_A) = (4, \infty)$ to child node $C$ and continue the search. The minimizing node $C$ has children $D$ with score 1, so $\beta_C = \min(1, \infty) = 1$, meaning that $C$ has value of at most 1.

Now, $\beta_C \leq \alpha_C$. This means that $C$ will have value at most $\beta_C$, but it will be smaller than the already-found value $\alpha_C$ passed from its parent. Therefore, we prune other children of $C$.

Now, let's consider the right tree in the example above. The node $A$ starts with $(\alpha_A, \beta_A) = (-\infty, \infty)$. It has a child with value $1$, and since $A$ is a minimizer, it updates $\beta_A := \min(1, \infty) = 1$. Then, it checks the second child $C$ which needs to check its children, so we assign its parameters $(\alpha_C, \beta_C) := (\alpha_A, \beta_A)$.

$C$ has child $D$ with value $0$, and since $C$ is a maximizing node, $\alpha_C := \max(0, -\infty) = 0$. Since $\beta_C \not\leq \alpha_C$, we continue. The next child $E$ has value $5$, so $\alpha_C := max(5, 0) = 5$. Now, $\beta_C \leq \alpha_C$, so we prune the rest of the children of $C$.

The process can be summarized by the pseudo-code below:

![Pseudocode of alpha-beta pruning]({{ "assets/mooc/aind/12/alphabeta_pseudocode.png" | absolute_url }})

Check [this video](https://youtu.be/l-hh51ncgDI?t=9m10s) for more explanation on updating alpha and beta.

### Optimizations for Isolation

Despite the size of game tree, it is possible to search to endgame through couple optimization techniques. One optimization is symmetry. Most boards at level 2 of the game tree are equivalent up to rotations and reflections. Instead of considering $25 \times 24 = 600$ board states, we need to  consider only $75$ board states.

![Example of a partitioned board in Isolation]({{ "assets/mooc/aind/12/isolation_partition.png" | absolute_url }})

Another optimization is using an evaluation function checking for partition. In the game of Isolation, the game is basically decided when the board is partitioned with players on different partitions.

### Opening Book

An **opening book** is a technique of keeping a large "book" of opening moves that are most likely to result in a win. These are often built by analyzing a large set of games played by expert human players or AI.

The game tree grows exponentially with the branching factor, and there can be noise in the outcome of the game, so it takes a lot of games to generate a reliable opening book.

Note that this technique can be used even when we are not using minimax search.

